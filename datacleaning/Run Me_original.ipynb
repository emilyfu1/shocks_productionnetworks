{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements to collpase requirements tables Note + Making Concordances Concord - Tony Gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "import os\n",
    "from data_cleaning_functions import requirements_clean, concordance_PCE_clean, \\\n",
    "    find_intermediate_industries, concordance_PCQ_clean, get_sales_from_make_matrix, clean_make_matrix, \\\n",
    "    get_demand_shock_from_shaipro_output, get_expenditure_weights_from_shapiro_outputs,plot_shapiro_graph_from_shapiro_ouput,clean_bea_PQE_table, get_final_demand_from_use_table\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "script_dir = str(Path().resolve().parent)\n",
    "file_path = os.path.join(script_dir) + \"/\" \n",
    "shapiro_file =  file_path + \"Shapiro\"\n",
    "raw_data_path = file_path + \"raw_bea_data\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Industry",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "PCE Expenditure",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "81262a12-1883-4991-b88c-17c46d9703fd",
       "rows": [
        [
         "5",
         "oilseed farming",
         null
        ],
        [
         "15",
         "forestry and logging",
         null
        ],
        [
         "18",
         "oil and gas extraction",
         null
        ],
        [
         "20",
         "copper, nickel, lead, and zinc mining",
         null
        ],
        [
         "21",
         "iron, gold, silver, and other metal ore mining",
         null
        ],
        [
         "22",
         "stone mining and quarrying",
         null
        ],
        [
         "24",
         "drilling oil and gas wells",
         null
        ],
        [
         "25",
         "other support activities for mining",
         null
        ],
        [
         "29",
         "health care structures",
         null
        ],
        [
         "30",
         "educational and vocational structures",
         null
        ],
        [
         "31",
         "nonresidential maintenance and repair",
         null
        ],
        [
         "32",
         "residential maintenance and repair",
         null
        ],
        [
         "33",
         "office and commercial structures",
         null
        ],
        [
         "34",
         "multifamily residential structures",
         null
        ],
        [
         "35",
         "other residential structures",
         null
        ],
        [
         "36",
         "manufacturing structures",
         null
        ],
        [
         "37",
         "other nonresidential structures",
         null
        ],
        [
         "38",
         "power and communication structures",
         null
        ],
        [
         "39",
         "single-family residential structures",
         null
        ],
        [
         "40",
         "transportation structures and highways and streets",
         null
        ],
        [
         "41",
         "sawmills and wood preservation",
         null
        ],
        [
         "42",
         "veneer, plywood, and engineered wood product manufacturing",
         null
        ],
        [
         "43",
         "millwork",
         null
        ],
        [
         "47",
         "cement manufacturing",
         null
        ],
        [
         "48",
         "ready-mix concrete manufacturing",
         null
        ],
        [
         "49",
         "concrete pipe, brick, and block manufacturing",
         null
        ],
        [
         "55",
         "mineral wool manufacturing",
         null
        ],
        [
         "56",
         "miscellaneous nonmetallic mineral products",
         null
        ],
        [
         "57",
         "iron and steel mills and ferroalloy manufacturing",
         null
        ],
        [
         "59",
         "alumina refining and primary aluminum production",
         null
        ],
        [
         "60",
         "aluminum product manufacturing from purchased aluminum",
         null
        ],
        [
         "61",
         "nonferrous metal (except aluminum) smelting and refining",
         null
        ],
        [
         "62",
         "copper rolling, drawing, extruding and alloying",
         null
        ],
        [
         "64",
         "ferrous metal foundries",
         null
        ],
        [
         "66",
         "custom roll forming",
         null
        ],
        [
         "67",
         "all other forging, stamping, and sintering",
         null
        ],
        [
         "71",
         "ornamental and architectural metal products manufacturing",
         null
        ],
        [
         "72",
         "power boiler and heat exchanger manufacturing",
         null
        ],
        [
         "73",
         "metal tank (heavy gauge) manufacturing",
         null
        ],
        [
         "77",
         "machine shops",
         null
        ],
        [
         "79",
         "coating, engraving, heat treating and allied activities",
         null
        ],
        [
         "80",
         "plumbing fixture fitting and trim manufacturing",
         null
        ],
        [
         "81",
         "valve and fittings other than plumbing",
         null
        ],
        [
         "82",
         "ball and roller bearing manufacturing",
         null
        ],
        [
         "83",
         "fabricated pipe and pipe fitting manufacturing",
         null
        ],
        [
         "86",
         "farm machinery and equipment manufacturing",
         null
        ],
        [
         "88",
         "construction machinery manufacturing",
         null
        ],
        [
         "89",
         "mining and oil and gas field machinery manufacturing",
         null
        ],
        [
         "90",
         "semiconductor machinery manufacturing",
         null
        ],
        [
         "97",
         "industrial and commercial fan and blower and air purification equipment manufacturing",
         null
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 143
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Industry</th>\n",
       "      <th>PCE Expenditure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oilseed farming</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>forestry and logging</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>oil and gas extraction</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>copper, nickel, lead, and zinc mining</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>iron, gold, silver, and other metal ore mining</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>federal general government (nondefense)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>other federal government enterprises</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>state and local government (educational services)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>state and local government (hospitals and heal...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>state and local government (other services)</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Industry PCE Expenditure\n",
       "5                                      oilseed farming             NaN\n",
       "15                                forestry and logging             NaN\n",
       "18                              oil and gas extraction             NaN\n",
       "20               copper, nickel, lead, and zinc mining             NaN\n",
       "21      iron, gold, silver, and other metal ore mining             NaN\n",
       "..                                                 ...             ...\n",
       "396            federal general government (nondefense)             NaN\n",
       "398               other federal government enterprises             NaN\n",
       "399  state and local government (educational services)             NaN\n",
       "400  state and local government (hospitals and heal...             NaN\n",
       "401        state and local government (other services)             NaN\n",
       "\n",
       "[143 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_table = pd.read_excel(os.path.join(raw_data_path, 'Use_SUT_Framework_2017_DET.xlsx'), sheet_name='2017')\n",
    "use_table_filtered = find_intermediate_industries(use_table)\n",
    "use_table_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Industry",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Unnamed: 424",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "67fae967-52b7-4184-9826-d45b8c62d22f",
       "rows": [
        [
         "5",
         "oilseed farming",
         "61204"
        ],
        [
         "6",
         "grain farming",
         "96281"
        ],
        [
         "7",
         "vegetable and melon farming",
         "50338"
        ],
        [
         "8",
         "fruit and tree nut farming",
         "82686"
        ],
        [
         "9",
         "greenhouse, nursery, and floriculture production",
         "44491"
        ],
        [
         "10",
         "other crop farming",
         "38538"
        ],
        [
         "11",
         "dairy cattle and milk production",
         "45989"
        ],
        [
         "12",
         "beef cattle ranching and farming, including feedlots and dual-purpose ranching and farming",
         "101458"
        ],
        [
         "13",
         "poultry and egg production",
         "53324"
        ],
        [
         "14",
         "animal production, except cattle and poultry and eggs",
         "50413"
        ],
        [
         "15",
         "forestry and logging",
         "31674"
        ],
        [
         "16",
         "fishing, hunting and trapping",
         "30450"
        ],
        [
         "17",
         "support activities for agriculture and forestry",
         "29859"
        ],
        [
         "18",
         "oil and gas extraction",
         "423371"
        ],
        [
         "19",
         "coal mining",
         "42616"
        ],
        [
         "20",
         "copper, nickel, lead, and zinc mining",
         "13889"
        ],
        [
         "21",
         "iron, gold, silver, and other metal ore mining",
         "20317"
        ],
        [
         "22",
         "stone mining and quarrying",
         "20470"
        ],
        [
         "23",
         "other nonmetallic mineral mining and quarrying",
         "28021"
        ],
        [
         "24",
         "drilling oil and gas wells",
         "28579"
        ],
        [
         "25",
         "other support activities for mining",
         "90739"
        ],
        [
         "26",
         "electric power generation, transmission, and distribution",
         "458193"
        ],
        [
         "27",
         "natural gas distribution",
         "80214"
        ],
        [
         "28",
         "water, sewage and other systems",
         "80967"
        ],
        [
         "29",
         "health care structures",
         "47123"
        ],
        [
         "30",
         "educational and vocational structures",
         "101181"
        ],
        [
         "31",
         "nonresidential maintenance and repair",
         "204428"
        ],
        [
         "32",
         "residential maintenance and repair",
         "84468"
        ],
        [
         "33",
         "office and commercial structures",
         "169386"
        ],
        [
         "34",
         "multifamily residential structures",
         "79772"
        ],
        [
         "35",
         "other residential structures",
         "252295"
        ],
        [
         "36",
         "manufacturing structures",
         "70668"
        ],
        [
         "37",
         "other nonresidential structures",
         "123210"
        ],
        [
         "38",
         "power and communication structures",
         "123629"
        ],
        [
         "39",
         "single-family residential structures",
         "270471"
        ],
        [
         "40",
         "transportation structures and highways and streets",
         "143054"
        ],
        [
         "41",
         "sawmills and wood preservation",
         "54181"
        ],
        [
         "42",
         "veneer, plywood, and engineered wood product manufacturing",
         "41599"
        ],
        [
         "43",
         "millwork",
         "35155"
        ],
        [
         "44",
         "all other wood product manufacturing",
         "40727"
        ],
        [
         "45",
         "clay product and refractory manufacturing",
         "23880"
        ],
        [
         "46",
         "glass and glass product manufacturing",
         "51149"
        ],
        [
         "47",
         "cement manufacturing",
         "12575"
        ],
        [
         "48",
         "ready-mix concrete manufacturing",
         "51196"
        ],
        [
         "49",
         "concrete pipe, brick, and block manufacturing",
         "9940"
        ],
        [
         "50",
         "other concrete product manufacturing",
         "19136"
        ],
        [
         "51",
         "lime and gypsum product manufacturing",
         "11274"
        ],
        [
         "52",
         "abrasive product manufacturing",
         "9326"
        ],
        [
         "53",
         "cut stone and stone product manufacturing",
         "11659"
        ],
        [
         "54",
         "ground or treated mineral and earth manufacturing",
         "6167"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 402
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Industry</th>\n",
       "      <th>Unnamed: 424</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>oilseed farming</td>\n",
       "      <td>61204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>grain farming</td>\n",
       "      <td>96281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vegetable and melon farming</td>\n",
       "      <td>50338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>fruit and tree nut farming</td>\n",
       "      <td>82686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>greenhouse, nursery, and floriculture production</td>\n",
       "      <td>44491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>other state and local government enterprises</td>\n",
       "      <td>107059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>scrap</td>\n",
       "      <td>46908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>used and secondhand goods</td>\n",
       "      <td>164495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>405</th>\n",
       "      <td>noncomparable imports</td>\n",
       "      <td>260421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>rest of the world adjustment</td>\n",
       "      <td>3441</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Industry Unnamed: 424\n",
       "5                                     oilseed farming        61204\n",
       "6                                       grain farming        96281\n",
       "7                         vegetable and melon farming        50338\n",
       "8                          fruit and tree nut farming        82686\n",
       "9    greenhouse, nursery, and floriculture production        44491\n",
       "..                                                ...          ...\n",
       "402      other state and local government enterprises       107059\n",
       "403                                             scrap        46908\n",
       "404                         used and secondhand goods       164495\n",
       "405                             noncomparable imports       260421\n",
       "406                      rest of the world adjustment         3441\n",
       "\n",
       "[402 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_table = pd.read_excel(os.path.join(raw_data_path, 'Use_SUT_Framework_2017_DET.xlsx'), sheet_name='2017')\n",
    "use_table = use_table.iloc[4:-11]\n",
    "use_table = use_table.loc[:, use_table.iloc[0].isin(['Commodity Description', 'T019'])]\n",
    "use_table = use_table.iloc[1:]\n",
    "use_table.rename(columns={'Unnamed: 1': 'Industry' , 'Unnamed: 405': 'PCE Expenditure'}, inplace=True)\n",
    "use_table.loc[use_table['Industry'] == 'Drugs and druggists’ sundries', 'Industry'] = 'Drugs and druggists sundries'\n",
    "use_table.loc[use_table['Industry'] == 'Insurance Carriers, except Direct Life Insurance', 'Industry'] = 'Insurance carriers, except direct life'\n",
    "use_table.loc[use_table['Industry'] == 'Tobacco product manufacturing', 'Industry'] = 'Tobacco manufacturing'\n",
    "use_table.loc[use_table['Industry'] == 'Scenic and sightseeing transportation and support activities for transportatio', 'Industry'] = 'scenic and sightseeing transportation and support activities'\n",
    "use_table.loc[use_table['Industry'] == 'Community food, housing, and other relief services, including rehabilitation services', 'Industry'] = 'community food, housing, and other relief services, including vocational rehabilitation services'\n",
    "use_table[\"Industry\"] = use_table[\"Industry\"].str.lower()\n",
    "use_table[\"Industry\"] = use_table[\"Industry\"].str.strip()\n",
    "use_table = use_table.dropna(subset=['Industry'])\n",
    "# # use_table = use_table[use_table['PCE Expenditure'].isna()]\n",
    "use_table \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Industry",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Unnamed: 424",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "PCE Expenditure",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "99145157-6503-42f7-afe0-c4a23d3796da",
       "rows": [
        [
         "0",
         "oilseed farming",
         "61204",
         null
        ],
        [
         "1",
         "forestry and logging",
         "31674",
         null
        ],
        [
         "2",
         "oil and gas extraction",
         "423371",
         null
        ],
        [
         "3",
         "copper, nickel, lead, and zinc mining",
         "13889",
         null
        ],
        [
         "4",
         "iron, gold, silver, and other metal ore mining",
         "20317",
         null
        ],
        [
         "5",
         "stone mining and quarrying",
         "20470",
         null
        ],
        [
         "6",
         "drilling oil and gas wells",
         "28579",
         null
        ],
        [
         "7",
         "other support activities for mining",
         "90739",
         null
        ],
        [
         "8",
         "health care structures",
         "47123",
         null
        ],
        [
         "9",
         "educational and vocational structures",
         "101181",
         null
        ],
        [
         "10",
         "nonresidential maintenance and repair",
         "204428",
         null
        ],
        [
         "11",
         "residential maintenance and repair",
         "84468",
         null
        ],
        [
         "12",
         "office and commercial structures",
         "169386",
         null
        ],
        [
         "13",
         "multifamily residential structures",
         "79772",
         null
        ],
        [
         "14",
         "other residential structures",
         "252295",
         null
        ],
        [
         "15",
         "manufacturing structures",
         "70668",
         null
        ],
        [
         "16",
         "other nonresidential structures",
         "123210",
         null
        ],
        [
         "17",
         "power and communication structures",
         "123629",
         null
        ],
        [
         "18",
         "single-family residential structures",
         "270471",
         null
        ],
        [
         "19",
         "transportation structures and highways and streets",
         "143054",
         null
        ],
        [
         "20",
         "sawmills and wood preservation",
         "54181",
         null
        ],
        [
         "21",
         "veneer, plywood, and engineered wood product manufacturing",
         "41599",
         null
        ],
        [
         "22",
         "millwork",
         "35155",
         null
        ],
        [
         "23",
         "cement manufacturing",
         "12575",
         null
        ],
        [
         "24",
         "ready-mix concrete manufacturing",
         "51196",
         null
        ],
        [
         "25",
         "concrete pipe, brick, and block manufacturing",
         "9940",
         null
        ],
        [
         "26",
         "mineral wool manufacturing",
         "9178",
         null
        ],
        [
         "27",
         "miscellaneous nonmetallic mineral products",
         "7431",
         null
        ],
        [
         "28",
         "iron and steel mills and ferroalloy manufacturing",
         "155649",
         null
        ],
        [
         "29",
         "alumina refining and primary aluminum production",
         "22021",
         null
        ],
        [
         "30",
         "aluminum product manufacturing from purchased aluminum",
         "35783",
         null
        ],
        [
         "31",
         "nonferrous metal (except aluminum) smelting and refining",
         "39059",
         null
        ],
        [
         "32",
         "copper rolling, drawing, extruding and alloying",
         "26138",
         null
        ],
        [
         "33",
         "ferrous metal foundries",
         "19589",
         null
        ],
        [
         "34",
         "custom roll forming",
         "7573",
         null
        ],
        [
         "35",
         "all other forging, stamping, and sintering",
         "14609",
         null
        ],
        [
         "36",
         "ornamental and architectural metal products manufacturing",
         "58946",
         null
        ],
        [
         "37",
         "power boiler and heat exchanger manufacturing",
         "13231",
         null
        ],
        [
         "38",
         "metal tank (heavy gauge) manufacturing",
         "10076",
         null
        ],
        [
         "39",
         "machine shops",
         "38090",
         null
        ],
        [
         "40",
         "coating, engraving, heat treating and allied activities",
         "26845",
         null
        ],
        [
         "41",
         "plumbing fixture fitting and trim manufacturing",
         "10987",
         null
        ],
        [
         "42",
         "valve and fittings other than plumbing",
         "55410",
         null
        ],
        [
         "43",
         "ball and roller bearing manufacturing",
         "11724",
         null
        ],
        [
         "44",
         "fabricated pipe and pipe fitting manufacturing",
         "7665",
         null
        ],
        [
         "45",
         "farm machinery and equipment manufacturing",
         "48582",
         null
        ],
        [
         "46",
         "construction machinery manufacturing",
         "64532",
         null
        ],
        [
         "47",
         "mining and oil and gas field machinery manufacturing",
         "32445",
         null
        ],
        [
         "48",
         "semiconductor machinery manufacturing",
         "22060",
         null
        ],
        [
         "49",
         "industrial and commercial fan and blower and air purification equipment manufacturing",
         "11165",
         null
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 143
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Industry</th>\n",
       "      <th>Unnamed: 424</th>\n",
       "      <th>PCE Expenditure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oilseed farming</td>\n",
       "      <td>61204</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>forestry and logging</td>\n",
       "      <td>31674</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oil and gas extraction</td>\n",
       "      <td>423371</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>copper, nickel, lead, and zinc mining</td>\n",
       "      <td>13889</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iron, gold, silver, and other metal ore mining</td>\n",
       "      <td>20317</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>federal general government (nondefense)</td>\n",
       "      <td>379143</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>other federal government enterprises</td>\n",
       "      <td>1233</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>state and local government (educational services)</td>\n",
       "      <td>832472</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>state and local government (hospitals and heal...</td>\n",
       "      <td>68698</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>state and local government (other services)</td>\n",
       "      <td>836043</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Industry Unnamed: 424  \\\n",
       "0                                      oilseed farming        61204   \n",
       "1                                 forestry and logging        31674   \n",
       "2                               oil and gas extraction       423371   \n",
       "3                copper, nickel, lead, and zinc mining        13889   \n",
       "4       iron, gold, silver, and other metal ore mining        20317   \n",
       "..                                                 ...          ...   \n",
       "138            federal general government (nondefense)       379143   \n",
       "139               other federal government enterprises         1233   \n",
       "140  state and local government (educational services)       832472   \n",
       "141  state and local government (hospitals and heal...        68698   \n",
       "142        state and local government (other services)       836043   \n",
       "\n",
       "    PCE Expenditure  \n",
       "0               NaN  \n",
       "1               NaN  \n",
       "2               NaN  \n",
       "3               NaN  \n",
       "4               NaN  \n",
       "..              ...  \n",
       "138             NaN  \n",
       "139             NaN  \n",
       "140             NaN  \n",
       "141             NaN  \n",
       "142             NaN  \n",
       "\n",
       "[143 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = use_table.merge(use_table_filtered, on=\"Industry\", how=\"inner\")\n",
    "merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in Requirements Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = pd.read_excel(os.path.join(raw_data_path, 'IxI_TR_2017_PRO_Det.xlsx'), sheet_name='2017')\n",
    "requirements = requirements_clean(requirements)\n",
    "requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = requirements.T\n",
    "requirements = requirements.fillna(0)\n",
    "delta = np.identity(len(requirements)) - np.linalg.inv(requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 (Making Concordances Concord) Adding Scrap, Used and secondhand goods and ROW adjustments to Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"These 4 industries are found in the concordance table but not in the Reqirements table so I am \n",
    "adding them manually with row and column inputs of zero\"\"\"\n",
    "concordance_but_not_requirments = [\"Scrap\", \"Used and secondhand goods\", \"Rest of the world adjustment\", \"noncomparable imports\"] \n",
    "delta = pd.DataFrame(delta, index=requirements.index, columns=requirements.columns)\n",
    "delta = delta.reindex(index=requirements.index.append(pd.Index(concordance_but_not_requirments)).str.lower() , columns=requirements.columns.append(pd.Index(concordance_but_not_requirments)).str.lower() , fill_value=0)\n",
    "delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5b Making Negative Values in Delta Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_count = (delta < 0).sum().sum()\n",
    "percent_of_negatives_before = negative_count/402**2\n",
    "delta[delta < 0] = 0\n",
    "negative_count_new = (delta < 0).sum().sum()\n",
    "percent_of_negatives_after = negative_count_new/402**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5b Force negatives to be zero but adjust row sums to be same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V = delta.sum(axis=0)\n",
    "# P = delta[delta > 0].sum()\n",
    "# delta.loc['norm'] = V/P\n",
    "# last_values = delta.iloc[-1]\n",
    "# delta[delta < 0] = 0\n",
    "# delta = delta.iloc[:-1].div(last_values)\n",
    "# delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealting with Intermediate Industries - Making Concordances Concord Section 3.3.1. - Operationalizing Industries without Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Making Concordances Concord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_table = pd.read_excel(os.path.join(raw_data_path, \"Use_SUT_Framework_2017_DET.xlsx\"), sheet_name=\"2017\")\n",
    "\n",
    "# Returns all industries with zero PCE \n",
    "intermediate_industries = find_intermediate_industries(use_table)\n",
    "\n",
    "intermediate_industries = intermediate_industries.iloc[:, [0]]\n",
    "intermediate_industries = intermediate_industries.sort_values(by=\"Industry\")\n",
    "intermediate_industries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps 2 - 5. Making Concordances Concord - Loop over Intermediate Industries and Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Loop \n",
    "big_lambda = np.eye(len(delta)) # create a big_lambda identity matrix that will become out final output\n",
    "big_lambda = pd.DataFrame(big_lambda)\n",
    "big_lambda = big_lambda.set_index(delta.index)\n",
    "big_lambda.columns = delta.index\n",
    "delta_industries = delta.index # save all the requirment table industries outside the loop \n",
    "removed = [] # stores all removed industries through the loop \n",
    "\n",
    "i = 0\n",
    "for row in intermediate_industries.iloc[::-1].itertuples(): # looping backwards\n",
    "    i += 1 \n",
    "    current = row.Industry # industry we are currently removing\n",
    "    # Create identity matrix for current loop iteration\n",
    "    phi_i = np.eye(len(delta) - (i - 1))\n",
    "    phi_i = pd.DataFrame(phi_i)\n",
    "    phi_i = phi_i.set_index(delta_industries)\n",
    "    phi_i.columns = delta_industries\n",
    "\n",
    "    current_intermediate_use_table = delta.copy() # use a new intermediates use matrix each time we do this\n",
    "    # Remove all the industries we've previously removed in earlier loop iterations \n",
    "    current_intermediate_use_table.drop(columns=removed, inplace=True)\n",
    "    current_intermediate_use_table = current_intermediate_use_table.loc[[current]] # the row of the intermediates use table associated with the current industry  \n",
    "    current_intermediate_use_table.drop(columns=[current], inplace=True) # drop the column associated with the current industry so we dont include it in our sum\n",
    "    current_intermediate_use_table = current_intermediate_use_table.astype(float)\n",
    "    current_industry_sum = current_intermediate_use_table.loc[current].sum() # find sum of row\n",
    "\n",
    "    # just making sure that we aren't dividing by zero \n",
    "    if current_industry_sum != 0:\n",
    "        current_intermediate_use_table = current_intermediate_use_table.astype(float)\n",
    "        current_intermediate_use_table.loc[current] = current_intermediate_use_table.loc[current] / current_industry_sum\n",
    "\n",
    "\n",
    "    current_intermediate_use_table = current_intermediate_use_table[sorted(current_intermediate_use_table.columns)] # sort the normazlied row alphabetically\n",
    "\n",
    "    # remove the current industry from intermediate list\n",
    "    intermediate_industries = intermediate_industries[intermediate_industries['Industry'] != current]\n",
    "    # drop the current industry column from phi_i and industry indexing\n",
    "    phi_i = phi_i.drop(columns=current)\n",
    "    delta_industries = delta_industries.drop(current) \n",
    "    \n",
    "    removed.append(current)\n",
    "\n",
    "    # Update phi_i with the current industry sale shares\n",
    "    phi_i.loc[current_intermediate_use_table.index] = current_intermediate_use_table.loc[current_intermediate_use_table.index].astype(np.float64).values\n",
    "    big_lambda_old = big_lambda\n",
    "    big_lambda = big_lambda @ phi_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Making Concordances Concord - Converting IO matrix from sales shares to dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_matrix = pd.read_excel(os.path.join(raw_data_path, \"Supply_2017_DET.xlsx\"), sheet_name=\"2017\")\n",
    "sales_vector = get_sales_from_make_matrix(make_matrix) # returns the sales for each industry\n",
    "\n",
    "# these industries are not in the make matrix so i will add them manually with zero entries to preserve df sizes\n",
    "industries_not_in_make_matrix = [\"state and local government passenger transit\", \"state and local government electric utilities\", \\\n",
    "                      \"secondary smelting and alloying of aluminum\", \"federal electric utilities\"]\n",
    "not_in_make_matrix = pd.DataFrame({'Industries': industries_not_in_make_matrix, 'Sales': [0,0,0,0]})\n",
    "sales_vector = pd.concat([sales_vector, not_in_make_matrix], ignore_index=True)\n",
    "sales_vector.set_index('Industries', inplace=True)\n",
    "\n",
    "# repeat the sales vector n times to make a sales matrix\n",
    "sales_repeated = pd.DataFrame({f'{i}': sales_vector['Sales'].values for i in range(1, len(delta))})\n",
    "sales_repeated.set_index(sales_vector.index, inplace=True)\n",
    "sales_matrix = pd.concat([sales_vector, sales_repeated], axis=1)\n",
    "sales_matrix.columns = delta.columns\n",
    "\n",
    "# Y is IO matrix in dollars \n",
    "Y = delta * sales_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Making Concordances Concord - Value Added for the Economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_Y = Y.sum(axis=1).to_frame()\n",
    "sum_Y.columns = ['Sales']\n",
    "VA_pre_transformation = (sales_vector - sum_Y).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8. Making Concordances Concord - IO Matrix in USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new IO matrix in USD \n",
    "Y_tilde = big_lambda.T @ Y @ big_lambda\n",
    "sales_tilde = big_lambda.T @ sales_vector\n",
    "\n",
    "# Verifing that economy wide value added is identical to the pre-transformation level\n",
    "sum_Y_tilde = Y_tilde.sum(axis=1).to_frame()\n",
    "sum_Y_tilde.columns = ['Sales']\n",
    "VA_post_transformation = (sales_tilde - sum_Y_tilde).sum()\n",
    "# VA_post_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pre transformation and post transformation are not the same\"\"\"\n",
    "differnece = VA_post_transformation - VA_pre_transformation\n",
    "differnece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9. Making Concordances Concord - Calculating New Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recip_sales = 1/sales_tilde\n",
    "sales_tilde_repeated = pd.DataFrame({f'{i}': recip_sales['Sales'].values for i in range(1, len(delta))})\n",
    "sales_tilde_repeated.set_index(recip_sales.index, inplace=True)\n",
    "sales_tilde_matrix = pd.concat([recip_sales, sales_tilde_repeated], axis=1)\n",
    "sales_tilde_matrix.columns = delta.columns\n",
    "\n",
    "delta_tilde = Y_tilde * sales_tilde_matrix\n",
    "delta = delta_tilde\n",
    "# delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Making Concordances Concord - Back to Requirements to Collapse Requirements Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Merge Concordance with Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance = pd.read_excel(os.path.join(raw_data_path, \"PCEBridge_2017_DET.xlsx\"), sheet_name=\"2017\")\n",
    "concordance = concordance_PCE_clean(concordance)\n",
    "# concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"'federal electric utilities', 'secondary smelting and alloying of aluminum', 'state and local government electric utilities', 'state and local government passenger transit' \n",
    "are not an intermediate industry nor do they exist in the PCE concordance so we will simply drop\"\"\"\n",
    "industries_in_requirments = set(delta.columns)\n",
    "indsutries_in_concordance = set(concordance[\"PCE Bridge Industries\"])\n",
    "industires_not_in_concordance = industries_in_requirments - indsutries_in_concordance\n",
    "delta = delta.drop(index=industires_not_in_concordance, columns=industires_not_in_concordance, errors='ignore')\n",
    "# delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_num_products = concordance['PCE Bridge Industries'].value_counts()\n",
    "\n",
    "delta_product_columns = pd.DataFrame(index=delta.index, columns=concordance['PCE Bridge Products'])\n",
    "\n",
    "for column in delta_product_columns: \n",
    "    industry = concordance.loc[concordance['PCE Bridge Products'] == column, 'PCE Bridge Industries'].values\n",
    "    delta_product_columns[column] = delta[industry] / in_num_products[industry]\n",
    "\n",
    "delta_product_cr = pd.DataFrame(columns=delta_product_columns.columns)\n",
    "lst_for_industry = []\n",
    "for row in delta_product_columns.index:\n",
    "    final_value = delta_product_columns.loc[row] / in_num_products[row]\n",
    "    products = concordance[concordance['PCE Bridge Industries'] == row]['PCE Bridge Products'].tolist()\n",
    "    for product in products: \n",
    "        lst_for_industry.append(row)\n",
    "        final_value_row = pd.DataFrame(final_value).T\n",
    "        final_value_row.index = [product]\n",
    "        \n",
    "        delta_product_cr = pd.concat([delta_product_cr, final_value_row])\n",
    "\n",
    "industry = pd.DataFrame(lst_for_industry, columns=['industry'])\n",
    "industry.index = delta_product_cr.index\n",
    "delta_product_cr = pd.concat([delta_product_cr, industry], axis = 1)\n",
    "# delta_product_cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Collapse columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_products = delta_product_cr.T.groupby(delta_product_cr.columns).sum().T\n",
    "# delta_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Make Matrix to Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_matrix = pd.read_excel(os.path.join(raw_data_path, \"Supply_2017_DET.xlsx\"), sheet_name='2017')\n",
    "sales = get_sales_from_make_matrix(make_matrix)\n",
    "# sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Merge concordance with sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_sales = pd.merge(concordance, sales, left_on='PCE Bridge Industries', right_on='Industries')\n",
    "product_sales = product_sales[[\"PCE Bridge Products\", \"Industries\", 'Sales']]\n",
    "# product_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Group sum of Sales for every Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the number of product categories an Industry belongs to \n",
    "product_sales['In#Products'] = product_sales['Industries'].map(product_sales['Industries'].value_counts())\n",
    "product_sales[\"Ratio_in_Product\"] = product_sales['Sales'] / product_sales['In#Products'] \n",
    "product_sales['Sales_Sum'] = product_sales.groupby('PCE Bridge Products')['Ratio_in_Product'].transform('sum')\n",
    "# product_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sales Share "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_sales['Sale Share'] = product_sales[\"Ratio_in_Product\"] / product_sales[\"Sales_Sum\"]\n",
    "product_sales = product_sales.rename(columns={'Industries': 'industry'})\n",
    "product_sales = product_sales.rename(columns={'PCE Bridge Products': 'product'})\n",
    "# product_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Merge Sale shares and Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset = delta_products.reset_index()\n",
    "delta_products = df_reset.rename(columns={'index': 'product'})\n",
    "delta_products_saleshare = delta_products.merge(product_sales, how = 'inner', on = ['industry', 'product'])\n",
    "delta_products_saleshare = delta_products_saleshare.drop(columns=['Sales_Sum', \"Ratio_in_Product\", \"In#Products\", \"Sales\", \"industry\"])\n",
    "delta_products_saleshare = delta_products_saleshare.set_index(delta_products_saleshare.columns[0])\n",
    "# delta_products_saleshare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. weightTimesDeltaValue + 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from Wide to Long\n",
    "delta_final = delta_products_saleshare.reset_index().melt(id_vars=[delta_products_saleshare.index.name, \"Sale Share\"],\n",
    "                                var_name=\"Column Products\", value_name=\"value\")\n",
    "\n",
    "# Just Renaming and Reordering\n",
    "delta_final = delta_final.rename(columns={delta_products_saleshare.index.name: \"Row Products\"})\n",
    "delta_final = delta_final[['Row Products', 'Column Products', 'value', 'Sale Share']]\n",
    "# Calculating weightTimesDeltaValue\n",
    "delta_final[\"weightTimesDeltaValue\"] = delta_final[\"value\"] * delta_final[\"Sale Share\"]\n",
    "\n",
    "# Sum weightTimesDeltaValue grouping by Row Products AND Column Products\n",
    "delta_final = delta_final.groupby(['Row Products', 'Column Products']).sum()\n",
    "delta_final\n",
    "# Convert Back to Wide Format \n",
    "delta_final = delta_final.pivot_table(values='weightTimesDeltaValue', index='Row Products', columns='Column Products')\n",
    "\n",
    "# Removes Index and Column Names cuz it looks better. Both Index and Columns are simply Product Categories now\n",
    "delta_final.columns.name = None \n",
    "delta_final.index.name = None \n",
    "\n",
    "\n",
    "labels_to_drop = [\"government employees' expenditures abroad\", \"private employees' expenditures abroad\",\"u.s. travel outside the united states\",\"u.s. student expenditures\"]\n",
    "delta_final = delta_final.drop(index=labels_to_drop, columns=labels_to_drop, errors=\"ignore\")\n",
    "\n",
    "# delta_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing Products with no price, quantity or expendiuture data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bea_quantities = pd.read_excel(os.path.join(raw_data_path, 'BEA Monthly Quantities.xlsx')) # read raw BEA quantities\n",
    "bea_quantities = clean_bea_PQE_table(bea_quantities, \"Quantities\", long=True) \n",
    "bea_quantities = bea_quantities[bea_quantities['products'].isin(delta_final.index)] # drop products taht delta doesnt have\n",
    "bea_quantities = bea_quantities.drop_duplicates()\n",
    "bea_quantities['Quantities'] = bea_quantities['Quantities'].replace('---', 0).astype(float)\n",
    "bea_quantities = bea_quantities.fillna(0)\n",
    "\n",
    "bea_prices = pd.read_excel(os.path.join(raw_data_path, 'BEA Monthly Prices.xlsx'))\n",
    "bea_prices = clean_bea_PQE_table(bea_prices, \"Prices\", long=True)\n",
    "bea_prices = bea_prices[bea_prices['products'].isin(delta_final.index)]\n",
    "bea_prices = bea_prices.drop_duplicates()\n",
    "bea_prices['Prices'] = bea_prices['Prices'].replace('---', 0).astype(float)\n",
    "bea_prices = bea_prices.fillna(0)\n",
    "\n",
    "bea_expenditures = pd.read_excel(os.path.join(raw_data_path, 'BEA Monthly Expenditures.xlsx'))\n",
    "bea_expenditures = clean_bea_PQE_table(bea_expenditures, \"Expenditures\", long=True)\n",
    "bea_expenditures = bea_expenditures[bea_expenditures['products'].isin(delta_final.index)]\n",
    "bea_expenditures = bea_expenditures.drop_duplicates()\n",
    "bea_expenditures['Expenditures'] = bea_expenditures['Expenditures'].replace('---', 0).astype(float)\n",
    "bea_expenditures = bea_expenditures.fillna(0)\n",
    "\n",
    "bea_PQE_merged = pd.merge(left=bea_quantities, right=bea_prices, on=['products', 'date'], how='outer')\n",
    "bea_PQE_merged = pd.merge(left=bea_PQE_merged, right=bea_expenditures, on=['products', 'date'], how='outer')\n",
    "\n",
    "# dropping this because 2024-10 has no data yet \n",
    "bea_PQE_merged = bea_PQE_merged.dropna(subset=['Expenditures'])\n",
    "\n",
    "dates = list(set(bea_expenditures[\"date\"]) & set(bea_prices[\"date\"]) & set(bea_quantities[\"date\"]))\n",
    "dates.sort() # get all BEA data monthly dates\n",
    "\n",
    "cols_to_check = ['Quantities', 'Prices', 'Expenditures']\n",
    "\n",
    "# Create filtered DataFrame where at least one value is missing or zero\n",
    "products_with_zero_somwhere = bea_PQE_merged[\n",
    "    bea_PQE_merged[cols_to_check].isnull().any(axis=1) |\n",
    "    (bea_PQE_merged[cols_to_check] == 0).any(axis=1)\n",
    "]\n",
    "\n",
    "all_bad_products_dict = {} # For every date products that dont have a value somewhere\n",
    "\n",
    "for date in dates:\n",
    "    bad_products_currnet = products_with_zero_somwhere[products_with_zero_somwhere['date'] == date][\"products\"]\n",
    "    all_bad_products_dict[date] = bad_products_currnet.tolist()\n",
    "\n",
    "\"\"\"\"For BEA I will set the entire BEA row to zero if any of the price, quantity or expenditure data is missing\"\"\"\n",
    "zero_mask = (bea_PQE_merged[cols_to_check] == 0).any(axis=1)\n",
    "# Set all three columns to 0 where the mask is True\n",
    "bea_PQE_merged.loc[zero_mask, cols_to_check] = 0\n",
    "\n",
    "bea_prices = bea_PQE_merged[[\"products\", \"date\", \"Prices\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Calculate Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = delta_final.sum(axis=1)\n",
    "gamma = pd.DataFrame(row_sums, columns=['Row_Sum'])\n",
    "gamma.index = delta_final.index \n",
    "gamma = gamma.apply(pd.to_numeric, errors='coerce')\n",
    "# gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Calculate Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_gamma = delta_final.merge(gamma, left_on=gamma.index, right_on=delta_final.index)\n",
    "delta_gamma = delta_gamma.set_index(delta_gamma.columns[0])\n",
    "row_sums = delta_gamma.iloc[:, -1]\n",
    "row_sums[row_sums == 0] = np.nan\n",
    "omega = delta_gamma.iloc[:, :-1].div(row_sums, axis=0)\n",
    "omega.columns.name = None \n",
    "omega.index.name = None \n",
    "omega = omega.apply(pd.to_numeric, errors='coerce')\n",
    "# omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different gamma and omega for missing products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_omega_monthly = {} # this will store a different gamma and omega for each month\n",
    "\n",
    "for date in all_bad_products_dict:\n",
    "\n",
    "    zero_products = all_bad_products_dict.get(date) # products that have no time series data for current month\n",
    "\n",
    "    current_gamma = gamma.copy()\n",
    "    current_delta = delta_final.copy()\n",
    "\n",
    "    for product in zero_products:\n",
    "        current_gamma.loc[current_gamma.index == product, 'Row_Sum'] = 0 # making missing sector expenditure 0 \n",
    "        if product in delta_final.columns: \n",
    "            current_delta[product] = 0 # missing time series products dont sell to any other sector\n",
    "            \n",
    "    current_row_sum = current_delta.sum(axis=1)\n",
    "\n",
    "    current_omega = current_delta.div(current_row_sum, axis=0)\n",
    "    checking_omega = current_omega\n",
    "\n",
    "    current_omega.columns.name = None \n",
    "    current_omega.index.name = None \n",
    "    current_omega = current_omega.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    gamma_omega_monthly[date] = [current_gamma, current_omega]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_omega_monthly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Doing our Supply/demand contribution graph properly (Which influenceer has hte most influence?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sales in product and time period\n",
    "sales = pd.DataFrame({'date': pd.Series(dtype='datetime64[ns]'),\n",
    "                   'products': pd.Series(dtype='str'),\n",
    "                   'sales': pd.Series(dtype='float')})\n",
    "\n",
    "for date in dates:\n",
    "\n",
    "    gamma = gamma_omega_monthly.get(date)[0]\n",
    "    omega = gamma_omega_monthly.get(date)[1]\n",
    " \n",
    "    # filter expenditures for the current date\n",
    "    expenditures_date = bea_expenditures[bea_expenditures['date'] == date][['products', 'Expenditures']].set_index('products')\n",
    "    expenditures_date = expenditures_date.sort_index()\n",
    "    expenditures_date = expenditures_date.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    gamma_series = gamma[\"Row_Sum\"]\n",
    "    diag_matrix = np.diag(gamma_series)\n",
    "\n",
    "    x = np.identity(len(omega)) - (omega.T @ diag_matrix)\n",
    "\n",
    "    sales_date = np.linalg.inv(x) @ expenditures_date\n",
    "\n",
    "    sales_date['date'] = date\n",
    "    sales_date['products'] = expenditures_date.index\n",
    "    sales_date.rename(columns={'Expenditures': 'sales'}, inplace=True)\n",
    "\n",
    "    sales = pd.concat([sales, sales_date], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Prices of Intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prices of intermediates (we use cobb douglas production with intermediates)\n",
    "intermediates = pd.DataFrame({'date': pd.Series(dtype='datetime64[ns]'),\n",
    "                   'products': pd.Series(dtype='str'),\n",
    "                   'intermediates': pd.Series(dtype='float')})\n",
    "for date in dates:\n",
    "\n",
    "    prices_date = bea_prices[bea_prices['date'] == date][['products', 'Prices']].set_index('products')\n",
    "    prices_date = prices_date.sort_index()\n",
    "\n",
    "    gamma = gamma_omega_monthly.get(date)[0]\n",
    "    omega = gamma_omega_monthly.get(date)[1]\n",
    "    \n",
    "    for i in gamma.index:\n",
    "        log_prices = np.log(prices_date['Prices'])\n",
    "        log_prices.replace(-np.inf, 0, inplace=True)        \n",
    "        intermediates.loc[len(intermediates)] = [date, i, np.exp(omega.loc[i] @ log_prices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Price of Value Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prices of value added\n",
    "value_added = pd.DataFrame({'date': pd.Series(dtype='datetime64[ns]'),\n",
    "                   'products': pd.Series(dtype='str'),\n",
    "                   'value_added': pd.Series(dtype='float')})\n",
    "\n",
    "for date in dates:\n",
    "    # filter prices for the current date\n",
    "    prices_date = bea_prices[bea_prices['date'] == date][['products', 'Prices']].set_index('products')\n",
    "    prices_date = prices_date.sort_index()\n",
    "\n",
    "    # filter intermediates for the current date\n",
    "    intermediates_date = intermediates[intermediates['date'] == date][['products', 'intermediates']].set_index('products')\n",
    "    intermediates_date = intermediates_date.sort_index()\n",
    "\n",
    "    gamma = gamma_omega_monthly.get(date)[0]    \n",
    "    gamma_series = gamma[\"Row_Sum\"]\n",
    "\n",
    "    value_added_date = np.exp((1/(1 - gamma_series.sort_index()))*(np.log(prices_date['Prices']) - gamma_series.sort_index() * np.log(intermediates_date['intermediates'])))\n",
    "\n",
    "    value_added_date = value_added_date.reset_index().rename(columns={0: 'value_added'})\n",
    "    value_added_date['date'] = date\n",
    "    value_added_date.rename(columns={\"index\": \"products\"}, inplace=True)\n",
    "\n",
    "    value_added = pd.concat([value_added, value_added_date], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales and Value Added VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 12\n",
    "residuals_part = []\n",
    "\n",
    "for product in gamma.index:\n",
    "    product_bad_dates = products_with_zero_somwhere[products_with_zero_somwhere[\"products\"] == product]\n",
    "    calculated = pd.merge(left=value_added, right=sales, on=['products', 'date'], how='inner')\n",
    "    calculated = calculated[calculated['products'] == product][['date', 'value_added', 'sales']].sort_values(['date'])\n",
    "    calculated = calculated.set_index('date')\n",
    "\n",
    "    if not product_bad_dates.empty: \n",
    "        calculated = calculated[~calculated.index.isin(product_bad_dates['date'])] # remove rows before we have all time series data\n",
    "    \n",
    "    calculated['value_added'] = np.log(calculated['value_added']).diff()\n",
    "    calculated['sales'] = np.log(calculated['sales']).diff()\n",
    "\n",
    "    calculated.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "\n",
    "    full_index_calc = pd.date_range(start=calculated.index.min(), end=calculated.index.max(), freq='MS')\n",
    "    calculated = calculated.reindex(full_index_calc)\n",
    "\n",
    "    model_calculated = VAR(calculated)\n",
    "    result_calculated = model_calculated.fit(lags)\n",
    "\n",
    "    residuals_calculated = result_calculated.resid.reset_index()\n",
    "    residuals_calculated['products'] = product\n",
    "    \n",
    "    residuals_part.append(residuals_calculated)\n",
    "\n",
    "IO_residuals = pd.concat(residuals_part, ignore_index=True)\n",
    "IO_residuals.rename(columns={'index': 'date', 'value_added': 'residual_value_added', 'sales': 'residual_sales'}, inplace=True)\n",
    "IO_residuals = IO_residuals.groupby(['date', 'products']).sum(min_count=1).reset_index()\n",
    "IO_residuals = IO_residuals.sort_values(['date', 'products'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price and Quantity VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 12\n",
    "residual_temp = []\n",
    "\n",
    "for product in gamma.index:\n",
    "\n",
    "    original = bea_PQE_merged[bea_PQE_merged['products'] == product][['date', 'Prices', 'Quantities']].sort_values(['date'])\n",
    "    original = original.set_index('date')\n",
    "\n",
    "    original.dropna(inplace=True)\n",
    "\n",
    "    original['Prices'] = np.log(original['Prices']).diff()\n",
    "    original['Quantities'] = np.log(original['Quantities']).diff()\n",
    "     \n",
    "\n",
    "    original.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    original.dropna(inplace=True)\n",
    "\n",
    "    full_index = pd.date_range(start=original.index.min(), end=original.index.max(), freq='MS')\n",
    "    original = original.reindex(full_index)\n",
    "\n",
    "    model_original = VAR(original)\n",
    "    result_original = model_original.fit(lags)\n",
    "\n",
    "    residuals_original = result_original.resid.reset_index()\n",
    "    residuals_original['products'] = product\n",
    "    \n",
    "    residual_temp.append(residuals_original)\n",
    "    \n",
    "residuals_normal = pd.concat(residual_temp, ignore_index=True)\n",
    "residuals_normal.rename(columns={'index': 'date', 'Prices': 'residual_prices', 'Quantities': 'residual_quantities'}, inplace=True)\n",
    "residuals_normal = residuals_normal.groupby(['date', 'products']).sum(min_count=1).reset_index()\n",
    "residuals_normal = residuals_normal.sort_values(['date', 'products'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Price of Value Added and Real Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_normal['majority_demand'] = ((residuals_normal['residual_prices'] * residuals_normal['residual_quantities']) >= 0).astype(int)\n",
    "residuals_normal['majority_supply'] = ((residuals_normal['residual_prices'] * residuals_normal['residual_quantities']) < 0).astype(int)\n",
    "\n",
    "IO_residuals['majority_demand_a'] = ((IO_residuals['residual_value_added'] * IO_residuals['residual_sales']) >= 0).astype(int)\n",
    "IO_residuals['majority_supply_a'] = ((IO_residuals['residual_value_added'] * IO_residuals['residual_sales']) < 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Rethinking Supply and Demand Influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Find N X T Value Added Prices and Output Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_P_VA = value_added.pivot(index='products', columns='date', values='value_added')\n",
    "log_P_VA = log_P_VA.applymap(lambda x: np.log(x) if x > 0 else 0)\n",
    "log_P_VA = log_P_VA.iloc[:, 12:]\n",
    "\n",
    "log_P = bea_prices.pivot(index='products', columns='date', values='Prices')\n",
    "log_P = log_P.applymap(lambda x: np.log(x) if x > 0 else 0)\n",
    "log_P = log_P.iloc[:, 12:]\n",
    "log_P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Get Big_Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm gonna make a different big_theta for every month based on that month's gamma and omega\n",
    "big_theta_monthly = {} # this will store a different big_theta for each month\n",
    "\n",
    "for date in dates: \n",
    "    \n",
    "    gamma = gamma_omega_monthly.get(date)[0]\n",
    "    omega = gamma_omega_monthly.get(date)[1]\n",
    "\n",
    "    # first matrix\n",
    "    step_1 = np.diag(gamma[\"Row_Sum\"]) @ omega\n",
    "    step_2 = np.eye(len(gamma)) - step_1\n",
    "    step_3 = np.linalg.inv(step_2)\n",
    "    # second matrix\n",
    "    step_4 = np.ones(len(gamma)) - gamma[\"Row_Sum\"]\n",
    "    step_5 = np.diag(step_4)\n",
    "    big_theta = step_3 @ step_5\n",
    "    big_theta = pd.DataFrame(big_theta, index=omega.index, columns=omega.columns)\n",
    "\n",
    "    big_theta_monthly[date] = big_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detour to Some possible checks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_1960 = big_theta_monthly.get(pd.Timestamp('1960-02-01 00:00:00'))\n",
    "theta_1983 = big_theta_monthly.get(pd.Timestamp('1983-09-01 00:00:00'))\n",
    "theta_2024 = big_theta_monthly.get(pd.Timestamp('2024-09-01 00:00:00'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get expenditure weights\n",
    "bea_expenditures = bea_expenditures.pivot(index='products', columns='date', values='Expenditures')\n",
    "expenditure_weights = bea_expenditures.div(bea_expenditures.sum(axis=0), axis=1)\n",
    "expenditure_weights = expenditure_weights.iloc[:, 13:]\n",
    "expenditure_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_weights[\"1960-02-01 00:00:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_weight_1960 = expenditure_weights[[\"1960-02-01 00:00:00\"]]\n",
    "expenditure_weight_1983 = expenditure_weights[[\"1983-01-01 00:00:00\"]]\n",
    "expenditure_weight_2024 = expenditure_weights[[\"2024-01-01 00:00:00\"]]\n",
    "\n",
    "# W_va_1960 = (theta_1960.T @ expenditure_weight_1960).sort_values(by=\"1960-02-01 00:00:00\",ascending=False)\n",
    "# W_va_1983 = (theta_1983.T @ expenditure_weight_1983).sort_values(by=\"1983-01-01 00:00:00\",ascending=False)\n",
    "# W_va_2024 = (theta_2024.T @ expenditure_weight_2024).sort_values(by=\"2024-01-01 00:00:00\",ascending=False)\n",
    "\n",
    "W_va_1960 = (theta_1960.T @ expenditure_weight_1960)\n",
    "W_va_1983 = (theta_1983.T @ expenditure_weight_1983)\n",
    "W_va_2024 = (theta_2024.T @ expenditure_weight_2024)\n",
    "\n",
    "\n",
    "\n",
    "expenditure_weight_1960_sorted = expenditure_weight_1960.sort_values(by=\"1960-02-01 00:00:00\",ascending=False)\n",
    "expenditure_weight_1983_sorted = expenditure_weight_1983.sort_values(by=\"1983-01-01 00:00:00\",ascending=False)\n",
    "expenditure_weight_2024_sorted = expenditure_weight_2024.sort_values(by=\"2024-01-01 00:00:00\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_weight_1983_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_va_1983"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.concat(\n",
    "    [expenditure_weight_1960_sorted.rename(columns={expenditure_weight_1960_sorted.columns[0]: \"expenditure_weight\"}),\n",
    "     W_va_1960.rename(columns={W_va_1960.columns[0]: \"value_added_weight\"})],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "ax = plot_df.plot(figsize=(20, 4))   # pandas uses matplotlib under the hood\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.set_title(\"Value_Added vs Expenditure Weight 1960\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.concat(\n",
    "    [expenditure_weight_1983_sorted.rename(columns={expenditure_weight_1983_sorted.columns[0]: \"expenditure_weight\"}),\n",
    "     W_va_1983.rename(columns={W_va_1983.columns[0]: \"value_added_weight\"})],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "ax = plot_df.plot(figsize=(20, 4))   # pandas uses matplotlib under the hood\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.set_title(\"Value_Added vs Expenditure Weight 1983\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.concat(\n",
    "    [expenditure_weight_2024_sorted.rename(columns={expenditure_weight_2024_sorted.columns[0]: \"expenditure_weight\"}),\n",
    "     W_va_2024.rename(columns={W_va_2024.columns[0]: \"value_added_weight\"})],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "ax = plot_df.plot(figsize=(20, 4))   # pandas uses matplotlib under the hood\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Value\")\n",
    "ax.set_title(\"Value_Added vs Expenditure Weight 2024\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) All entries are positive or zero 🙂"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_theta = {} # Stores True if any big_theta contians a negative for a given month\n",
    "\n",
    "for date in dates: \n",
    "    current_big_theta = big_theta_monthly.get(date)\n",
    "    psotive = (current_big_theta < 0).any().any()\n",
    "    negative_theta[date] = psotive\n",
    "\n",
    "any(negative_theta.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Row Sum does equal one 😃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sum_not_one = {} # store True if a big_theta column sum isnt 1 for a given month\n",
    "\n",
    "for date in dates: \n",
    "    current_big_theta = big_theta_monthly.get(date)\n",
    "    current_row_sums = current_big_theta.sum(axis=1).to_frame()\n",
    "    is_one = current_row_sums.iloc[:, 0].sub(1).abs().le(0.001).all()\n",
    "    row_sum_not_one[date] = is_one\n",
    "\n",
    "any(not v for v in row_sum_not_one.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c) log(p) == big_theta @ log(p)_VA 🤠"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_log_p_check = [] # we will make a matrix where we use the big_theta for a given month @ log_P_VA then store the column corresponding to that month\n",
    "\n",
    "for date in dates[12:]: \n",
    "    current_big_theta = big_theta_monthly.get(date)\n",
    "    theta_log_p_month = current_big_theta @ log_P_VA\n",
    "    theta_log_p = theta_log_p_month[date]\n",
    "    theta_log_p_check.append(theta_log_p)\n",
    "\n",
    "theta_log_p_check_df = pd.concat(theta_log_p_check, axis=1)\n",
    "\n",
    "diff = (log_P - theta_log_p_check_df).abs() # take the differnce \n",
    "\n",
    "mask = diff > 0.00000000001 # acceptable difference threshold \n",
    "\n",
    "mismatch_locs = mask.stack()\n",
    "mismatches_df = pd.DataFrame({\n",
    "    'log_P': log_P.stack()[mismatch_locs],\n",
    "    'check_against_log_p': theta_log_p_check_df.stack()[mismatch_locs],\n",
    "    'abs_diff': diff.stack()[mismatch_locs]\n",
    "})\n",
    "\n",
    "# Returns postions where difference between the two dfs are greater than threshold\n",
    "mismatches_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Find log_P_VA|D=1 and log_P_VA|D=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IO_cassification = IO_residuals.pivot(index='products', columns='date', values='majority_demand_a')\n",
    "log_P_VA_D1 = log_P_VA * IO_cassification\n",
    "log_P_VA_D0 = log_P_VA * (np.ones((IO_cassification.shape[0], IO_cassification.shape[1])) - IO_cassification)\n",
    "\n",
    "# im gonna fill Nans with 0 \n",
    "log_P_VA_D1_filled = log_P_VA_D1.fillna(0)\n",
    "log_P_VA_D0_filled = log_P_VA_D0.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_P_VA_D1_filled_1960 = log_P_VA_D1_filled.loc[log_P_VA_D1_filled[\"1960-02-01 00:00:00\"] != 0,[\"1960-02-01 00:00:00\"]]\n",
    "log_P_VA_D1_filled_1983 = log_P_VA_D1_filled.loc[log_P_VA_D1_filled[\"1983-01-01 00:00:00\"] != 0,[\"1983-01-01 00:00:00\"]]\n",
    "log_P_VA_D1_filled_2024 = log_P_VA_D1_filled.loc[log_P_VA_D1_filled[\"2024-01-01 00:00:00\"] != 0,[\"2024-01-01 00:00:00\"]]\n",
    "\n",
    "log_P_VA_D0_filled_1960 = log_P_VA_D0_filled.loc[log_P_VA_D0_filled[\"1960-02-01 00:00:00\"] != 0,[\"1960-02-01 00:00:00\"]]\n",
    "log_P_VA_D0_filled_1983 = log_P_VA_D0_filled.loc[log_P_VA_D0_filled[\"1983-01-01 00:00:00\"] != 0,[\"1983-01-01 00:00:00\"]]\n",
    "log_P_VA_D0_filled_2024 = log_P_VA_D0_filled.loc[log_P_VA_D0_filled[\"2024-01-01 00:00:00\"] != 0,[\"2024-01-01 00:00:00\"]]\n",
    "\n",
    "\n",
    "def smooth_counts(counts, window=5):\n",
    "    return (\n",
    "        pd.Series(counts)\n",
    "        .rolling(window, center=True, min_periods=1)\n",
    "        .mean()\n",
    "        .to_numpy()\n",
    "    )\n",
    "\n",
    "def plot_smoothed_hist_overlay(s1, s2, label1=\"series1\", label2=\"series2\", bins=80, title=\"\"):\n",
    "    x1 = np.asarray(s1).ravel()\n",
    "    x2 = np.asarray(s2).ravel()\n",
    "\n",
    "    xmin = min(x1.min(), x2.min())\n",
    "    xmax = max(x1.max(), x2.max())\n",
    "    edges = np.linspace(xmin, xmax, bins + 1)\n",
    "\n",
    "    counts1, _ = np.histogram(x1, bins=edges, density=False)\n",
    "    counts2, _ = np.histogram(x2, bins=edges, density=False)\n",
    "\n",
    "    centers = (edges[:-1] + edges[1:]) / 2\n",
    "\n",
    "    smooth1 = smooth_counts(counts1, window=5)\n",
    "    smooth2 = smooth_counts(counts2, window=5)\n",
    "\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(centers, smooth1, label=label1, alpha=0.8)\n",
    "    plt.fill_between(centers, smooth1, alpha=0.3)\n",
    "\n",
    "    plt.plot(centers, smooth2, label=label2, alpha=0.8)\n",
    "    plt.fill_between(centers, smooth2, alpha=0.3)\n",
    "\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.ylabel(\"Count (smoothed)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_smoothed_hist_overlay(log_P_VA_D1_filled_1960, log_P_VA_D0_filled_1960, label1=\"D1\", label2=\"D0\", title=\"1960 not normalized\")\n",
    "plot_smoothed_hist_overlay(log_P_VA_D1_filled_1983, log_P_VA_D0_filled_1983, label1=\"D1\", label2=\"D0\", title=\"1983 not normalized\")\n",
    "plot_smoothed_hist_overlay(log_P_VA_D1_filled_2024, log_P_VA_D0_filled_2024, label1=\"D1\", label2=\"D0\", title=\"2024 not normalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_va = value_added.pivot(index='products', columns='date', values='value_added')\n",
    "\n",
    "p_va_1960 = p_va[\"1960-02-01 00:00:00\"]\n",
    "p_va_1983 = p_va[[\"1983-01-01 00:00:00\"]]\n",
    "p_va_2024 = p_va[[\"2024-01-01 00:00:00\"]]\n",
    "\n",
    "# Import Raw Price again and clean\n",
    "prices = pd.read_excel(os.path.join(raw_data_path, 'BEA Monthly Prices.xlsx'))\n",
    "prices = clean_bea_PQE_table(prices, \"prices\")\n",
    "# Filter for the 210 BEA products that we actually use\n",
    "prices = prices.loc[prices.index.intersection(p_va_2024.index)]\n",
    "prices = prices[~prices.index.duplicated(keep='first')]\n",
    "prices = prices.reset_index()\n",
    "\n",
    "# Find month over month product specific infaltion starting form 1960-02\n",
    "prices_long = pd.melt(prices, id_vars='products', var_name='month', value_name='prices')\n",
    "prices_long['prices'] = pd.to_numeric(prices_long['prices'], errors='coerce')\n",
    "def month_over_month_product_inflation(df):\n",
    "    \"\"\"Function to Calculate month over month product specific inflation\"\"\"\n",
    "    # Ensure sorted by product and time\n",
    "    df = df.sort_values(by=['products', 'month'])\n",
    "    # Group by product and compute year-over-year inflation\n",
    "    df['inflation percent'] = df.groupby('products')['prices'].transform(lambda x: ((x - x.shift(1)) / x.shift(1)))\n",
    "        \n",
    "    return df\n",
    "inflation = month_over_month_product_inflation(prices_long)\n",
    "inflation = inflation.pivot(index='products', columns='month', values='inflation percent')\n",
    "inflation = inflation.iloc[:, 13:]\n",
    "\n",
    "inflation_1960 = inflation[[\"1960-02-01 00:00:00\"]]\n",
    "inflation_1983 = inflation[[\"1983-01-01 00:00:00\"]]\n",
    "inflation_2024 = inflation[[\"2024-01-01 00:00:00\"]]\n",
    "\n",
    "# contribution_1960 = ((W_va_1960 * p_va_1960) / inflation_1960).replace([np.inf, -np.inf], 0) \n",
    "# contribution_1983 = ((W_va_1983 * p_va_1983) / inflation_1983).replace([np.inf, -np.inf], 0) \n",
    "# contribution_2024 = ((W_va_2024 * p_va_2024) / inflation_2024).replace([np.inf, -np.inf], 0) \n",
    "\n",
    "contribution_1960 = ((W_va_1960.T @ np.diag(p_va_1960))).T / (inflation_1960 * expenditure_weight_1960).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contribution_1960.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. log_P_VA_D0 + log_P_VA_D1 == log_P_VA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_P_VA_D01 = log_P_VA_D0 + log_P_VA_D1\n",
    "log_P_VA_D01 = log_P_VA_D01.fillna(0)\n",
    "\n",
    "diff = (log_P_VA_D01 - log_P_VA).abs() # take the differnce \n",
    "mask = diff > 0.00000001 # acceptable difference threshold \n",
    "mismatch_locs = mask.stack()\n",
    "mismatches_df = pd.DataFrame({\n",
    "    'log_P_VA_D01': log_P_VA_D01.stack()[mismatch_locs],\n",
    "    'log_P_VA': log_P_VA.stack()[mismatch_locs],\n",
    "    'abs_diff': diff.stack()[mismatch_locs]\n",
    "})\n",
    "# Returns postions where difference between the two dfs are greater than threshold\n",
    "mismatches_df # Mismatches are where var data is lagged and thus missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. log_P == big_theta * [log_P_VA_D0 + log_P_VA_D1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_2 = [] \n",
    "\n",
    "for date in dates[12:]: \n",
    "    current_big_theta = big_theta_monthly.get(date)\n",
    "    current_check_2 = current_big_theta @ log_P_VA_D01\n",
    "    check_2_date = current_check_2[date]\n",
    "    check_2.append(check_2_date)\n",
    "\n",
    "check_2_df = pd.concat(check_2, axis=1)\n",
    "\n",
    "diff = (log_P_VA_D01 - log_P_VA).abs() # take the differnce \n",
    "mask = diff > 0.00000001 # acceptable difference threshold \n",
    "mismatch_locs = mask.stack()\n",
    "mismatches_df = pd.DataFrame({\n",
    "    'log_P': log_P.stack()[mismatch_locs],\n",
    "    'check_against_log_p': log_P_VA.stack()[mismatch_locs],\n",
    "    'abs_diff': diff.stack()[mismatch_locs]\n",
    "})\n",
    "# Returns postions where difference between the two dfs are greater than threshold\n",
    "mismatches_df # Mismatches are where var data is lagged and thus missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Find log_P|D=1 and log_P|D=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_P_D1 = []\n",
    "log_P_D0 = []\n",
    "\n",
    "for date in dates[12:]: \n",
    "\n",
    "    log_p_D1_current =  big_theta_monthly.get(date) @ log_P_VA_D1_filled\n",
    "    log_p_D1_month = log_p_D1_current[date]\n",
    "    \n",
    "    log_p_D0_current =  big_theta_monthly.get(date) @ log_P_VA_D0_filled\n",
    "    log_p_D0_month = log_p_D0_current[date]\n",
    "\n",
    "    log_P_D1.append(log_p_D1_month)\n",
    "    log_P_D0.append(log_p_D0_month)\n",
    "\n",
    "\n",
    "log_P_D1 = pd.concat(log_P_D1, axis=1)\n",
    "log_P_D0 = pd.concat(log_P_D0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log_P_VA|D=1 + log_P_VA|D=0 == log(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_against_log_p = log_P_D0 + log_P_D1\n",
    "check_against_log_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = (log_P - check_against_log_p).abs()\n",
    "\n",
    "mask = diff > 1\n",
    "\n",
    "mismatch_locs = mask.stack()\n",
    "\n",
    "mismatches_df = pd.DataFrame({\n",
    "    'log_P': log_P.stack()[mismatch_locs],\n",
    "    'check_against_log_p': check_against_log_p.stack()[mismatch_locs],\n",
    "    'abs_diff': diff.stack()[mismatch_locs]\n",
    "})\n",
    "mismatches_df # Mismatches are where var data is lagged and thus missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Calculate Final D Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# log_P contains zeros so I will manually change inf to zeros. \n",
    "D_influence = log_P_D1 / log_P \n",
    "D_influence.replace([float('inf'), float('-inf')], 0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check that All elements of D are between 0 and 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((D_influence.dropna() >= 0) & (D_influence.dropna() <= 1)).all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(data, plot_title, plot_text):\n",
    "    \"\"\"Function to Graph Stacked Plot\"\"\"\n",
    "    data = data.loc[data.index >= pd.Timestamp('1970-01-01')]\n",
    "\n",
    "    supply_inflation = data[[\"annual_supply_inflation\"]].copy()\n",
    "    supply_inflation.rename(columns={'annual_supply_inflation': 'Supply Inflation'}, inplace=True)\n",
    "\n",
    "    demand_inflation = data[[\"annual_demand_inflation\"]].copy()\n",
    "    demand_inflation.rename(columns={'annual_demand_inflation': 'Demand Inflation'}, inplace=True)\n",
    "\n",
    "    supply_inflation['supply_pos'] = supply_inflation['Supply Inflation'].apply(lambda x: x if x > 0 else 0)\n",
    "    demand_inflation['demand_pos'] = demand_inflation['Demand Inflation'].apply(lambda x: x if x > 0 else 0)\n",
    "    supply_inflation['supply_neg'] = supply_inflation['Supply Inflation'].apply(lambda x: x if x < 0 else 0)\n",
    "    demand_inflation['demand_neg'] = demand_inflation['Demand Inflation'].apply(lambda x: x if x < 0 else 0)\n",
    "\n",
    "    demand_inflation = demand_inflation.iloc[:-1]\n",
    "    supply_inflation = supply_inflation.iloc[:-1]\n",
    "\n",
    "    plt.figure(figsize=(26, 12))\n",
    "\n",
    "    plt.stackplot(supply_inflation.index, demand_inflation['demand_pos'], supply_inflation['supply_pos'], colors= [\"#008000\", \"#FF0000\"], labels = [\"Deamnd\", \"Supply\"])\n",
    "    plt.stackplot(supply_inflation.index, demand_inflation['demand_neg'], supply_inflation['supply_neg'], colors= [\"#008000\", \"#FF0000\"])\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Inflation')\n",
    "    plt.title(f'{plot_title}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.text(0.02, 0.95, \n",
    "         f'{plot_text}', \n",
    "         transform=plt.gca().transAxes, fontsize=9,\n",
    "         bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inflation Graphs A La Shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO VAR classifictions\n",
    "D_IO = IO_residuals.pivot(index='products', columns='date', values='majority_demand_a')\n",
    "# Shapiro VAR classifictions\n",
    "D_shapiro = residuals_normal.pivot(index='products', columns='date', values='majority_demand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_over_month_product_inflation(df):\n",
    "    \"\"\"Function to Calculate month over month product specific inflation\"\"\"\n",
    "    # Ensure sorted by product and time\n",
    "    df = df.sort_values(by=['products', 'month'])\n",
    "    # Group by product and compute year-over-year inflation\n",
    "    df['inflation percent'] = df.groupby('products')['prices'].transform(lambda x: ((x - x.shift(1)) / x.shift(1)))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Product Level month over month price changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Raw Price again and clean\n",
    "prices = pd.read_excel(os.path.join(raw_data_path, 'BEA Monthly Prices.xlsx'))\n",
    "prices = clean_bea_PQE_table(prices, \"prices\")\n",
    "# Filter for the 210 BEA products that we actually use\n",
    "prices = prices.loc[prices.index.intersection(D_shapiro.index)]\n",
    "prices = prices[~prices.index.duplicated(keep='first')]\n",
    "prices = prices.reset_index()\n",
    "\n",
    "# Find month over month product specific infaltion starting form 1960-02\n",
    "prices_long = pd.melt(prices, id_vars='products', var_name='month', value_name='prices')\n",
    "prices_long['prices'] = pd.to_numeric(prices_long['prices'], errors='coerce')\n",
    "inflation = month_over_month_product_inflation(prices_long)\n",
    "inflation = inflation.pivot(index='products', columns='month', values='inflation percent')\n",
    "inflation = inflation.iloc[:, 13:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get expenditure share for each product per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get expenditure weights\n",
    "bea_expenditures = bea_expenditures.pivot(index='products', columns='date', values='Expenditures')\n",
    "expenditure_weights = bea_expenditures.div(bea_expenditures.sum(axis=0), axis=1)\n",
    "expenditure_weights = expenditure_weights.iloc[:, 13:]\n",
    "expenditure_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use IO and Shapiro Supply and Demand Classifications to get Demand and Supply Driven Inflation based on product price changes and expenditure weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO Classification Aggregated Monthly Inflation\n",
    "inflation_parts_IO = {}\n",
    "\n",
    "for date in expenditure_weights:\n",
    "    current_demand = D_IO[str(date)] # all products calssified as demand for this period\n",
    "    current_supply = 1 - current_demand # all products calssified as supply for this period\n",
    "\n",
    "    current_weights = expenditure_weights[date] # all product weights for this period\n",
    "    current_inflation = inflation[date] # all product specific inflation for this period\n",
    "\n",
    "    demand_inflation = (current_demand * current_weights * current_inflation).sum() # Aggregated montly demand driven inflation\n",
    "    supply_inflation = (current_supply * current_weights * current_inflation).sum() # Aggregated montly supply driven inflation\n",
    "\n",
    "    inflation_parts_IO[date] = [demand_inflation, supply_inflation] # Store each month's data\n",
    "\n",
    "# Final demand and supply driven inflation\n",
    "inflation_IO_final = pd.DataFrame(inflation_parts_IO, index=['demand_inflation', 'supply_inflation'])\n",
    "inflation_IO_final = inflation_IO_final.T\n",
    "\n",
    "# Shapiro Classification Aggregated Monthly Inflation\n",
    "inflation_parts_shapiro = {}\n",
    "\n",
    "for date in expenditure_weights:\n",
    "    current_demand = D_shapiro[str(date)]\n",
    "    current_supply = 1 - current_demand\n",
    "\n",
    "    current_weights = expenditure_weights[date]\n",
    "    current_inflation = inflation[date]\n",
    "\n",
    "    demand_inflation = (current_demand * current_weights * current_inflation).sum()\n",
    "    supply_inflation = (current_supply * current_weights * current_inflation).sum()\n",
    "\n",
    "    inflation_parts_shapiro[date] = [demand_inflation, supply_inflation]\n",
    "\n",
    "\n",
    "inflation_shapiro_final = pd.DataFrame(inflation_parts_shapiro, index=['demand_inflation', 'supply_inflation'])\n",
    "inflation_shapiro_final = inflation_shapiro_final.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Anual Supply and Deamnd Driven Inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO Classification - Anual Supply Inflation\n",
    "inflation_IO_final['annual_supply_inflation'] = (\n",
    "    (inflation_IO_final['supply_inflation'].add(1).shift(1).rolling(window=12, min_periods=12).apply(np.prod, raw=True) - 1) * 100\n",
    ")\n",
    "# IO Classification - Anual Demand Inflation\n",
    "inflation_IO_final['annual_demand_inflation'] = (\n",
    "    (inflation_IO_final['demand_inflation'].add(1).shift(1).rolling(window=12, min_periods=12).apply(np.prod, raw=True) - 1) * 100\n",
    ")\n",
    "# Shapiro Classification - Anual Supply Inflation\n",
    "inflation_shapiro_final['annual_supply_inflation'] = (\n",
    "    (inflation_shapiro_final['supply_inflation'].add(1).shift(1).rolling(window=12, min_periods=12).apply(np.prod, raw=True) - 1) * 100\n",
    ")\n",
    "# Shapiro Classification - Anual Demand Inflation\n",
    "inflation_shapiro_final['annual_demand_inflation'] = (\n",
    "    (inflation_shapiro_final['demand_inflation'].add(1).shift(1).rolling(window=12, min_periods=12).apply(np.prod, raw=True) - 1) * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(inflation_shapiro_final, \"Shapiro Classification - Inflation Calculated a la Shapiro\", \"• Using our 210 products \\n• Using Price and Quantities for Residuals and Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(inflation_IO_final, \"ShapirIO Classification - Inflation Calculated a la Shapiro\", \"• Using our 210 products \\n• Using Value-Added and Sales for Residuals and Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from Shapiro's Code to Recreate the Exact Graph from his paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_code_output = pd.read_excel(os.path.join(shapiro_file, 'shaprio_stata_output_excel.xlsx'))\n",
    "shapiro_graph = plot_shapiro_graph_from_shapiro_ouput(shapiro_code_output, \"Shapiro Graph Using His 130 Products, Data as Found in His Paper\")\n",
    "shapiro_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. An Alternative way to Calculate Inflation - Log Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_inflation_althernative(df):\n",
    "    \"\"\"Function to Calculate month over month product specific inflation using log differneces\"\"\"\n",
    "    # Ensure sorted by product and time\n",
    "    df = df.sort_values(by=['products', 'month'])\n",
    "    # Group by product and compute year-over-year inflation\n",
    "    df['inflation log differnece'] = df.groupby('products')['prices'].transform(lambda x: (np.log(x) - np.log(x.shift(1))))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get log difference month over month product specific price changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation_log_difference = month_inflation_althernative(prices_long)\n",
    "inflation_log_difference = inflation_log_difference.pivot(index='products', columns='month', values='inflation log differnece')\n",
    "inflation_log_difference = inflation_log_difference.iloc[:, 13:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expenditure_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use IO and Shapiro Supply and Demand Classifications to get Demand and Supply Driven Inflation based on product price changes and expenditure weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO Classification Aggregated Monthly Inflation\n",
    "inflation_log_parts_IO = {}\n",
    "\n",
    "for date in expenditure_weights:\n",
    "    current_demand = D_IO[str(date)] # all products calssified as demand for this period\n",
    "    current_supply = 1 - current_demand # all products calssified as supply for this period\n",
    "\n",
    "    current_weights = expenditure_weights[date] # all product weights for this period\n",
    "    current_inflation = inflation_log_difference[date] # all product specific inflation for this period\n",
    "\n",
    "    demand_inflation = (current_demand * current_weights * current_inflation).sum() # Aggregated montly demand driven inflation\n",
    "    supply_inflation = (current_supply * current_weights * current_inflation).sum() # Aggregated montly supply driven inflation\n",
    "\n",
    "    inflation_log_parts_IO[date] = [demand_inflation, supply_inflation] # Store each month's data\n",
    "\n",
    "# Final demand and supply driven inflation\n",
    "inflation_log_IO_final = pd.DataFrame(inflation_log_parts_IO, index=['demand_inflation_log', 'supply_inflation_log'])\n",
    "inflation_log_IO_final = inflation_log_IO_final.T\n",
    "\n",
    "# Shapiro Classification Aggregated Monthly Inflation\n",
    "inflation_log_parts_shapiro = {}\n",
    "\n",
    "for date in expenditure_weights:\n",
    "    current_demand = D_shapiro[str(date)]\n",
    "    current_supply = 1 - current_demand\n",
    "\n",
    "    current_weights = expenditure_weights[date]\n",
    "    current_inflation = inflation_log_difference[date]\n",
    "\n",
    "    demand_inflation = (current_demand * current_weights * current_inflation).sum()\n",
    "    supply_inflation = (current_supply * current_weights * current_inflation).sum()\n",
    "\n",
    "    inflation_log_parts_shapiro[date] = [demand_inflation, supply_inflation]\n",
    "\n",
    "\n",
    "inflation_log_shapiro_final = pd.DataFrame(inflation_log_parts_shapiro, index=['demand_inflation_log', 'supply_inflation_log'])\n",
    "inflation_log_shapiro_final = inflation_log_shapiro_final.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Anual Supply and Deamnd Driven Inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IO Classification - Anual Supply Inflation\n",
    "inflation_log_IO_final['annual_supply_inflation'] = (\n",
    "    (inflation_log_IO_final['supply_inflation_log'].rolling(window=12, min_periods=12).sum()) * 100\n",
    ")\n",
    "# IO Classification - Anual Demand Inflation\n",
    "inflation_log_IO_final['annual_demand_inflation'] = (\n",
    "    (inflation_log_IO_final['demand_inflation_log'].rolling(window=12, min_periods=12).sum()) * 100\n",
    ")\n",
    "# Shapiro Classification - Anual Supply Inflation\n",
    "inflation_log_shapiro_final['annual_supply_inflation'] = (\n",
    "    (inflation_log_shapiro_final['supply_inflation_log'].rolling(window=12, min_periods=12).sum()) * 100\n",
    ")\n",
    "# Shapiro Classification - Anual Demand Inflation\n",
    "inflation_log_shapiro_final['annual_demand_inflation'] = (\n",
    "    (inflation_log_shapiro_final['demand_inflation_log'].rolling(window=12, min_periods=12).sum()) * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(inflation_log_IO_final, \"ShapirIO Classification - Inflation Log Difference\", \"• Using our 210 products \\n• Using Value-Added and Sales for Residuals and Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(inflation_log_shapiro_final, \"Shapiro Classification - Inflation Log Difference\", \"• Using our 210 products \\n• Using Price and Quantities for Residuals and Classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Using D Influence Instead of Dummy Variable for Demand and Supply Driven Inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Inflation a la Shapiro\"\"\"\n",
    "inflation_parts_IO_influence = {}\n",
    "\n",
    "for date in expenditure_weights:\n",
    "    current_demand = D_influence[str(date)] # all products calssified as demand for this period\n",
    "    current_supply = 1 - current_demand # all products calssified as supply for this period\n",
    "\n",
    "    current_weights = expenditure_weights[date] # all product weights for this period\n",
    "    current_inflation = inflation[date] # all product specific inflation for this period\n",
    "\n",
    "    demand_inflation = (current_demand * current_weights * current_inflation).sum() # Aggregated montly demand driven inflation\n",
    "    supply_inflation = (current_supply * current_weights * current_inflation).sum() # Aggregated montly supply driven inflation\n",
    "\n",
    "    inflation_parts_IO_influence[date] = [demand_inflation, supply_inflation] # Store each month's data \n",
    "\n",
    "# Final demand and supply driven inflation\n",
    "inflation_IO_influence = pd.DataFrame(inflation_parts_IO_influence, index=['demand_inflation', 'supply_inflation'])\n",
    "inflation_IO_influence = inflation_IO_influence.T\n",
    "\n",
    "\"\"\"Inflation Log Difference\"\"\"\n",
    "inflation_parts_IO_log = {}\n",
    "\n",
    "for date in expenditure_weights:\n",
    "    current_demand = D_influence[str(date)] # all products calssified as demand for this period\n",
    "    current_supply = 1 - current_demand # all products calssified as supply for this period\n",
    "\n",
    "    current_weights = expenditure_weights[date] # all product weights for this period\n",
    "    current_inflation = inflation_log_difference[date] # all product specific inflation for this period\n",
    "\n",
    "    demand_inflation = (current_demand * current_weights * current_inflation).sum() # Aggregated montly demand driven inflation\n",
    "    supply_inflation = (current_supply * current_weights * current_inflation).sum() # Aggregated montly supply driven inflation\n",
    "\n",
    "    inflation_parts_IO_log[date] = [demand_inflation, supply_inflation] # Store each month's data\n",
    "\n",
    "# Final demand and supply driven inflation\n",
    "inflation_IO_influence_log = pd.DataFrame(inflation_parts_IO_log, index=['demand_inflation_log', 'supply_inflation_log'])\n",
    "inflation_IO_influence_log = inflation_IO_influence_log.T\n",
    "\n",
    "# Anual Supply Inflation - a la Shapiro\n",
    "inflation_IO_influence['annual_supply_inflation'] = (\n",
    "    (inflation_IO_influence['supply_inflation'].add(1).shift(1).rolling(window=12, min_periods=12).apply(np.prod, raw=True) - 1) * 100\n",
    ")\n",
    "# Anual Demand Inflation - a la Shapiro\n",
    "inflation_IO_influence['annual_demand_inflation'] = (\n",
    "    (inflation_IO_influence['demand_inflation'].add(1).shift(1).rolling(window=12, min_periods=12).apply(np.prod, raw=True) - 1) * 100\n",
    ")\n",
    "# Anual Supply Inflation - Log Difference\n",
    "inflation_IO_influence_log['annual_supply_inflation'] = (\n",
    "    (inflation_IO_influence_log['supply_inflation_log'].rolling(window=12, min_periods=12).sum()) * 100\n",
    ")\n",
    "# Anual Demand Inflation - Log Difference \n",
    "inflation_IO_influence_log['annual_demand_inflation'] = (\n",
    "    (inflation_IO_influence_log['demand_inflation_log'].rolling(window=12, min_periods=12).sum()) * 100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(inflation_IO_influence, \"ShapirIO Influence Matrix - Inflation a la Shapiro\", \"• Using our 210 products \\n• Using Value-Added and Sales for Residuals and Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(inflation_IO_influence_log, \"ShapirIO Influence Matrix - Inflation Log Difference\", \"• Using our 210 products \\n• Using Value-Added and Sales for Residuals and Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation_IO_influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation_IO_final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
