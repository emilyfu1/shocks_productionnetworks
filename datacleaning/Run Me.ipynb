{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Requirements to collpase requirements tables Note + Making Concordances Concord - Tony Gui"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values, find_dotenv\n",
    "from statsmodels.tsa.api import VAR\n",
    "import os\n",
    "import re\n",
    "from data_cleaning_functions import requirements_clean, concordance_PCE_clean, \\\n",
    "    find_intermediate_industries, concordance_PCQ_clean, get_sales_from_make_matrix, clean_make_matrix, \\\n",
    "    get_demand_shock_from_shaipro_output, get_expenditure_weights_from_shapiro_outputs,plot_shapiro_graph_from_shapiro_ouput,clean_bea_PQE_table\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, message=\".*concatenation with empty or all-NA entries is deprecated.*\")\n",
    "\n",
    "script_dir = str(Path().resolve().parent)\n",
    "file_path = os.path.join(script_dir) + \"/\" \n",
    "shapiro_file =  file_path + \"Shapiro\"\n",
    "raw_data_path = file_path + \"raw_bea_data\" \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in Requirements Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = pd.read_excel(os.path.join(raw_data_path, 'IxI_TR_2017_PRO_Det.xlsx'), sheet_name='2017')\n",
    "requirements = requirements_clean(requirements)\n",
    "requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements = requirements.T\n",
    "with pd.option_context(\"future.no_silent_downcasting\", True):\n",
    "    requirements = requirements.fillna(0).infer_objects(copy=False)\n",
    "delta = np.identity(len(requirements)) - np.linalg.inv(requirements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 (Making Concordances Concord) Adding Scrap, Used and secondhand goods and ROW adjustments to Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"These 4 industries are found in the concordance table but not in the Reqirements table so I am \n",
    "adding them manually with row and column inputs of zero\"\"\"\n",
    "concordance_but_not_requirments = [\"Scrap\", \"Used and secondhand goods\", \"Rest of the world adjustment\", \"noncomparable imports\"] \n",
    "delta = pd.DataFrame(delta, index=requirements.index, columns=requirements.columns)\n",
    "delta = delta.reindex(index=requirements.index.append(pd.Index(concordance_but_not_requirments)).str.lower() , columns=requirements.columns.append(pd.Index(concordance_but_not_requirments)).str.lower() , fill_value=0)\n",
    "\n",
    "delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5b Making Negative Values in Delta Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_count = (delta < 0).sum().sum()\n",
    "x = negative_count/402**2\n",
    "delta[delta < 0] = 0\n",
    "negative_count_new = (delta < 0).sum().sum()\n",
    "y = negative_count_new/402**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5b Force negatives to be zero but adjust row sums to be same as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V = delta.sum(axis=0)\n",
    "# P = delta[delta > 0].sum()\n",
    "# delta.loc['norm'] = V/P\n",
    "# last_values = delta.iloc[-1]\n",
    "# delta[delta < 0] = 0\n",
    "# delta = delta.iloc[:-1].div(last_values)\n",
    "# delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealting with Intermediate Industries - Making Concordances Concord Section 3.3.1. - Operationalizing Industries without Products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Making Concordances Concord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_table = pd.read_excel(os.path.join(raw_data_path, \"Use_SUT_Framework_2017_DET.xlsx\"), sheet_name=\"2017\")\n",
    "# peq_concordance = pd.read_excel(os.path.join(raw_data_path, \"PEQBridge_2017_DET.xlsx\"), sheet_name=\"2017\")\n",
    "\n",
    "# Returns all industries with zero PCE \n",
    "intermediate_industries = find_intermediate_industries(use_table)\n",
    "\n",
    "# peq_concordance = concordance_PCQ_clean(peq_concordance)\n",
    "# intermediate_industries = intermediate_industries[~intermediate_industries['Industry'].isin(peq_concordance['Industries'])]\n",
    "\n",
    "intermediate_industries = intermediate_industries.iloc[:, [0]]\n",
    "intermediate_industries = intermediate_industries.sort_values(by=\"Industry\")\n",
    "intermediate_industries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps 2 - 5. Making Concordances Concord - Loop over Intermediate Industries and Remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements = pd.read_excel(os.path.join(raw_data_path, 'IxI_TR_2017_PRO_Det.xlsx'), sheet_name='2017')\n",
    "# requirements = requirements_clean(requirements, wide=True)\n",
    "\n",
    "# For the Loop \n",
    "big_lambda = np.eye(len(delta)) # create a big_lambda identity matrix that will become out final output\n",
    "big_lambda = pd.DataFrame(big_lambda)\n",
    "big_lambda = big_lambda.set_index(delta.index)\n",
    "big_lambda.columns = delta.index\n",
    "delta_industries = delta.index # save all the requirment table industries outside the loop \n",
    "removed = [] # stores all removed industries through the loop \n",
    "\n",
    "i = 0\n",
    "for row in intermediate_industries.iloc[::-1].itertuples(): # looping backwards\n",
    "    i += 1 \n",
    "    current = row.Industry # industry we are currently removing\n",
    "    # Create identity matrix for current loop iteration\n",
    "    phi_i = np.eye(len(delta) - (i - 1))\n",
    "    phi_i = pd.DataFrame(phi_i)\n",
    "    phi_i = phi_i.set_index(delta_industries)\n",
    "    phi_i.columns = delta_industries\n",
    "\n",
    "    current_intermediate_use_table = delta.copy() # use a new intermediates use matrix each time we do this\n",
    "    # Remove all the industries we've previously removed in earlier loop iterations \n",
    "    current_intermediate_use_table.drop(columns=removed, inplace=True)\n",
    "    current_intermediate_use_table = current_intermediate_use_table.loc[[current]] # the row of the intermediates use table associated with the current industry  \n",
    "    current_intermediate_use_table.drop(columns=[current], inplace=True) # drop the column associated with the current industry so we dont include it in our sum\n",
    "    current_intermediate_use_table = current_intermediate_use_table.astype(float)\n",
    "    current_industry_sum = current_intermediate_use_table.loc[current].sum() # find sum of row\n",
    "\n",
    "    # just making sure that we aren't dividing by zero \n",
    "    if current_industry_sum != 0:\n",
    "        current_intermediate_use_table = current_intermediate_use_table.astype(float)\n",
    "        current_intermediate_use_table.loc[current] = current_intermediate_use_table.loc[current] / current_industry_sum\n",
    "\n",
    "\n",
    "    current_intermediate_use_table = current_intermediate_use_table[sorted(current_intermediate_use_table.columns)] # sort the normazlied row alphabetically\n",
    "\n",
    "    # remove the current industry from intermediate list\n",
    "    intermediate_industries = intermediate_industries[intermediate_industries['Industry'] != current]\n",
    "    # drop the current industry column from phi_i and industry indexing\n",
    "    phi_i = phi_i.drop(columns=current)\n",
    "    delta_industries = delta_industries.drop(current) \n",
    "    \n",
    "    removed.append(current)\n",
    "\n",
    "    # Update phi_i with the current industry sale shares\n",
    "    phi_i.loc[current_intermediate_use_table.index] = current_intermediate_use_table.loc[current_intermediate_use_table.index].astype(np.float64).values\n",
    "    big_lambda_old = big_lambda\n",
    "    big_lambda = big_lambda @ phi_i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6. Making Concordances Concord - Converting IO matrix from sales shares to dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_matrix = pd.read_excel(os.path.join(raw_data_path, \"Supply_2017_DET.xlsx\"), sheet_name=\"2017\")\n",
    "sales_vector = get_sales_from_make_matrix(make_matrix) # returns the sales for each industry\n",
    "\n",
    "# these industries are not in the make matrix so i will add them manually with zero entries to preserve df sizes\n",
    "industries_not_in_make_matrix = [\"state and local government passenger transit\", \"state and local government electric utilities\", \\\n",
    "                      \"secondary smelting and alloying of aluminum\", \"federal electric utilities\"]\n",
    "not_in_make_matrix = pd.DataFrame({'Industries': industries_not_in_make_matrix, 'Sales': [0,0,0,0]})\n",
    "sales_vector = pd.concat([sales_vector, not_in_make_matrix], ignore_index=True)\n",
    "sales_vector.set_index('Industries', inplace=True)\n",
    "\n",
    "# repeat the sales vector n times to make a sales matrix\n",
    "sales_repeated = pd.DataFrame({f'{i}': sales_vector['Sales'].values for i in range(1, len(delta))})\n",
    "sales_repeated.set_index(sales_vector.index, inplace=True)\n",
    "sales_matrix = pd.concat([sales_vector, sales_repeated], axis=1)\n",
    "sales_matrix.columns = delta.columns\n",
    "\n",
    "# Y is IO matrix in dollars \n",
    "Y = delta * sales_matrix \n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7. Making Concordances Concord - Value Added for the Economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_Y = Y.sum(axis=1).to_frame()\n",
    "sum_Y.columns = ['Sales']\n",
    "VA_pre_transformation = (sales_vector - sum_Y).sum()\n",
    "\n",
    "VA_pre_transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8. Making Concordances Concord - IO Matrix in USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new IO matrix in USD \n",
    "Y_tilde = big_lambda.T @ Y @ big_lambda\n",
    "sales_tilde = big_lambda.T @ sales_vector\n",
    "\n",
    "# Verifing that economy wide value added is identical to the pre-transformation level\n",
    "sum_Y_tilde = Y_tilde.sum(axis=1).to_frame()\n",
    "sum_Y_tilde.columns = ['Sales']\n",
    "VA_post_transformation = (sales_tilde - sum_Y_tilde).sum()\n",
    "VA_post_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pre transformation and post transformation are not the same\"\"\"\n",
    "\n",
    "differnece = VA_post_transformation - VA_pre_transformation\n",
    "differnece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9. Making Concordances Concord - Calculating New Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recip_sales = 1/sales_tilde\n",
    "sales_tilde_repeated = pd.DataFrame({f'{i}': recip_sales['Sales'].values for i in range(1, len(delta))})\n",
    "sales_tilde_repeated.set_index(recip_sales.index, inplace=True)\n",
    "sales_tilde_matrix = pd.concat([recip_sales, sales_tilde_repeated], axis=1)\n",
    "sales_tilde_matrix.columns = delta.columns\n",
    "\n",
    "delta_tilde = Y_tilde * sales_tilde_matrix\n",
    "delta = delta_tilde\n",
    "delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Making Concordances Concord - Back to Requirements to Collapse Requirements Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Merge Concordance with Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concordance = pd.read_excel(os.path.join(raw_data_path, \"PCEBridge_2017_DET.xlsx\"), sheet_name=\"2017\")\n",
    "concordance = concordance_PCE_clean(concordance)\n",
    "concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the problem here is some industries in the requirments matrix still have no match to PCE they are matching to PEQ so i must drop them here when im trying to \n",
    "make the requirments table in terms of products however im not sure if this changes some of the math performed to the requirments matrix ie delta later on\n",
    "\"\"\"\n",
    "industries_in_requirments = set(delta.columns)\n",
    "indsutries_in_concordance = set(concordance[\"PCE Bridge Industries\"])\n",
    "industires_not_in_concordance = industries_in_requirments - indsutries_in_concordance\n",
    "delta = delta.drop(index=industires_not_in_concordance, columns=industires_not_in_concordance, errors='ignore')\n",
    "delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_num_products = concordance['PCE Bridge Industries'].value_counts()\n",
    "\n",
    "delta_product_columns = pd.DataFrame(index=delta.index, columns=concordance['PCE Bridge Products'])\n",
    "\n",
    "for column in delta_product_columns: \n",
    "    industry = concordance.loc[concordance['PCE Bridge Products'] == column, 'PCE Bridge Industries'].values\n",
    "    delta_product_columns[column] = delta[industry] / in_num_products[industry]\n",
    "\n",
    "delta_product_cr = pd.DataFrame(columns=delta_product_columns.columns)\n",
    "lst_for_industry = []\n",
    "for row in delta_product_columns.index:\n",
    "    final_value = delta_product_columns.loc[row] / in_num_products[row]\n",
    "    products = concordance[concordance['PCE Bridge Industries'] == row]['PCE Bridge Products'].tolist()\n",
    "    for product in products: \n",
    "        lst_for_industry.append(row)\n",
    "        final_value_row = pd.DataFrame(final_value).T\n",
    "        final_value_row.index = [product]\n",
    "        \n",
    "        delta_product_cr = pd.concat([delta_product_cr, final_value_row])\n",
    "\n",
    "industry = pd.DataFrame(lst_for_industry, columns=['industry'])\n",
    "industry.index = delta_product_cr.index\n",
    "delta_product_cr = pd.concat([delta_product_cr, industry], axis = 1)\n",
    "delta_product_cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Collapse columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_products = delta_product_cr.T.groupby(delta_product_cr.columns).sum().T\n",
    "delta_products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Make Matrix to Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_matrix = pd.read_excel(os.path.join(raw_data_path, \"Supply_2017_DET.xlsx\"), sheet_name='2017')\n",
    "sales = get_sales_from_make_matrix(make_matrix)\n",
    "sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Merge concordance with sales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_sales = pd.merge(concordance, sales, left_on='PCE Bridge Industries', right_on='Industries')\n",
    "product_sales = product_sales[[\"PCE Bridge Products\", \"Industries\", 'Sales']]\n",
    "product_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Group sum of Sales for every Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the number of product categories an Industry belongs to \n",
    "product_sales['In#Products'] = product_sales['Industries'].map(product_sales['Industries'].value_counts())\n",
    "product_sales[\"Ratio_in_Product\"] = product_sales['Sales'] / product_sales['In#Products'] \n",
    "product_sales['Sales_Sum'] = product_sales.groupby('PCE Bridge Products')['Ratio_in_Product'].transform('sum')\n",
    "product_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Sales Share "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_sales['Sale Share'] = product_sales[\"Ratio_in_Product\"] / product_sales[\"Sales_Sum\"]\n",
    "product_sales = product_sales.rename(columns={'Industries': 'industry'})\n",
    "product_sales = product_sales.rename(columns={'PCE Bridge Products': 'product'})\n",
    "product_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Merge Sale shares and Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reset = delta_products.reset_index()\n",
    "delta_products = df_reset.rename(columns={'index': 'product'})\n",
    "delta_products_saleshare = delta_products.merge(product_sales, how = 'inner', on = ['industry', 'product'])\n",
    "delta_products_saleshare = delta_products_saleshare.drop(columns=['Sales_Sum', \"Ratio_in_Product\", \"In#Products\", \"Sales\", \"industry\"])\n",
    "delta_products_saleshare = delta_products_saleshare.set_index(delta_products_saleshare.columns[0])\n",
    "delta_products_saleshare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. weightTimesDeltaValue + 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from Wide to Long\n",
    "delta_final = delta_products_saleshare.reset_index().melt(id_vars=[delta_products_saleshare.index.name, \"Sale Share\"],\n",
    "                                var_name=\"Column Products\", value_name=\"value\")\n",
    "\n",
    "# Just Renaming and Reordering\n",
    "delta_final = delta_final.rename(columns={delta_products_saleshare.index.name: \"Row Products\"})\n",
    "delta_final = delta_final[['Row Products', 'Column Products', 'value', 'Sale Share']]\n",
    "# Calculating weightTimesDeltaValue\n",
    "delta_final[\"weightTimesDeltaValue\"] = delta_final[\"value\"] * delta_final[\"Sale Share\"]\n",
    "\n",
    "# Sum weightTimesDeltaValue grouping by Row Products AND Column Products\n",
    "delta_final = delta_final.groupby(['Row Products', 'Column Products']).sum()\n",
    "delta_final\n",
    "# Convert Back to Wide Format \n",
    "delta_final = delta_final.pivot_table(values='weightTimesDeltaValue', index='Row Products', columns='Column Products')\n",
    "\n",
    "# Removes Index and Column Names cuz it looks better. Both Index and Columns are simply Product Categories now\n",
    "delta_final.columns.name = None \n",
    "delta_final.index.name = None \n",
    "\n",
    "\n",
    "labels_to_drop = [\"government employees' expenditures abroad\", \"private employees' expenditures abroad\",\"u.s. travel outside the united states\",\"u.s. student expenditures\"]\n",
    "delta_final = delta_final.drop(index=labels_to_drop, columns=labels_to_drop, errors=\"ignore\")\n",
    "\n",
    "delta_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixing Products with no price, quantity or expendiuture data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bea_quantities = pd.read_excel(os.path.join(raw_data_path, 'BEA Monthly Quantities.xlsx'))\n",
    "bea_quantities = clean_bea_PQE_table(bea_quantities, \"Quantities\", long=True)\n",
    "bea_quantities = bea_quantities[bea_quantities['products'].isin(delta_final.index)]\n",
    "bea_quantities = bea_quantities.drop_duplicates()\n",
    "bea_quantities['Quantities'] = bea_quantities['Quantities'].replace('---', 0).astype(float)\n",
    "bea_quantities = bea_quantities.fillna(0)\n",
    "\n",
    "bea_prices = pd.read_excel(os.path.join(raw_data_path, 'BEA Monthly Prices.xlsx'))\n",
    "bea_prices = clean_bea_PQE_table(bea_prices, \"Prices\", long=True)\n",
    "bea_prices = bea_prices[bea_prices['products'].isin(delta_final.index)]\n",
    "bea_prices = bea_prices.drop_duplicates()\n",
    "bea_prices['Prices'] = bea_prices['Prices'].replace('---', 0).astype(float)\n",
    "bea_prices = bea_prices.fillna(0)\n",
    "\n",
    "bea_expenditures = pd.read_excel(os.path.join(raw_data_path, 'BEA Monthly Expenditures.xlsx'))\n",
    "bea_expenditures = clean_bea_PQE_table(bea_expenditures, \"Expenditures\", long=True)\n",
    "bea_expenditures = bea_expenditures[bea_expenditures['products'].isin(delta_final.index)]\n",
    "bea_expenditures = bea_expenditures.drop_duplicates()\n",
    "bea_expenditures['Expenditures'] = bea_expenditures['Expenditures'].replace('---', 0).astype(float)\n",
    "bea_expenditures = bea_expenditures.fillna(0)\n",
    "\n",
    "bea_PQE_merged = pd.merge(left=bea_quantities, right=bea_prices, on=['products', 'date'], how='outer')\n",
    "bea_PQE_merged = pd.merge(left=bea_PQE_merged, right=bea_expenditures, on=['products', 'date'], how='outer')\n",
    "\n",
    "# dropping this because 2024-10 has no data yet \n",
    "bea_PQE_merged = bea_PQE_merged.dropna(subset=['Expenditures'])\n",
    "\n",
    "dates = list(set(bea_expenditures[\"date\"]) & set(bea_prices[\"date\"]) & set(bea_quantities[\"date\"]))\n",
    "dates.sort()\n",
    "\n",
    "cols_to_check = ['Quantities', 'Prices', 'Expenditures']\n",
    "\n",
    "# Create filtered DataFrame where at least one value is missing or zero\n",
    "products_with_zero_somwhere = bea_PQE_merged[\n",
    "    bea_PQE_merged[cols_to_check].isnull().any(axis=1) |\n",
    "    (bea_PQE_merged[cols_to_check] == 0).any(axis=1)\n",
    "]\n",
    "\n",
    "all_bad_products_dict = {}\n",
    "\n",
    "for date in dates:\n",
    "    bad_products_currnet = products_with_zero_somwhere[products_with_zero_somwhere['date'] == date][\"products\"]\n",
    "    all_bad_products_dict[date] = bad_products_currnet.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Calculate Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = delta_final.sum(axis=1)\n",
    "gamma = pd.DataFrame(row_sums, columns=['Row_Sum'])\n",
    "gamma.index = delta_final.index \n",
    "gamma = gamma.apply(pd.to_numeric, errors='coerce')\n",
    "gamma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13. Calculate Omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_gamma = delta_final.merge(gamma, left_on=gamma.index, right_on=delta_final.index)\n",
    "delta_gamma = delta_gamma.set_index(delta_gamma.columns[0])\n",
    "row_sums = delta_gamma.iloc[:, -1]\n",
    "row_sums[row_sums == 0] = np.nan\n",
    "omega = delta_gamma.iloc[:, :-1].div(row_sums, axis=0)\n",
    "omega.columns.name = None \n",
    "omega.index.name = None \n",
    "omega = omega.apply(pd.to_numeric, errors='coerce')\n",
    "omega"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different gamma and omega for missing products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_omega_yearly = {} # this will store a different gamma and omega for each month\n",
    "\n",
    "for date in all_bad_products_dict:\n",
    "\n",
    "    zero_products = all_bad_products_dict.get(date) # products that have no time series data for current month\n",
    "\n",
    "    current_gamma = gamma.copy()\n",
    "    current_delta = delta_final.copy()\n",
    "\n",
    "    for product in zero_products:\n",
    "        current_gamma.loc[current_gamma.index == product, 'Row_Sum'] = 0 # making missing sector sell 0 to very other sector\n",
    "        if product in delta_final.columns: \n",
    "            current_delta[product] = 0 # rescale omega for each month so it sums to 1 \n",
    "            \n",
    "    current_row_sum = current_delta.sum(axis=1)\n",
    "\n",
    "    current_omega = current_delta.div(current_row_sum, axis=0)\n",
    "    current_omega.columns.name = None \n",
    "    current_omega.index.name = None \n",
    "    current_omega = current_omega.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    gamma_omega_yearly[date] = [current_gamma, current_omega]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Doing our Supply/demand contribution graph properly (Which influenceer has hte most influence?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_code_output = pd.read_excel(os.path.join(shapiro_file, 'shaprio_stata_output_excel.xlsx'))\n",
    "haver_code_concordance = pd.read_excel(os.path.join(shapiro_file, 'Haver Codes to Product Names.xlsx'))\n",
    "mapping = dict(zip(haver_code_concordance['Shapiro Price Name'], haver_code_concordance['PCE Category']))\n",
    "\"\"\" I dont know why but in Shapiro's output the expenditure for water transport is titled s\"\"\"\n",
    "mapping[\"s\"] = \"Water transportation (65)\"\n",
    "expenditure_weights = get_expenditure_weights_from_shapiro_outputs(shapiro_code_output, mapping)\n",
    "demand_shock = get_demand_shock_from_shaipro_output(shapiro_code_output, mapping)\n",
    "# shapiro_graph = plot_shapiro_graph_from_shapiro_ouput(shapiro_code_output, \"Shapiro Graph\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sales in product and time period\n",
    "sales = pd.DataFrame({'date': pd.Series(dtype='datetime64[ns]'),\n",
    "                   'products': pd.Series(dtype='str'),\n",
    "                   'sales': pd.Series(dtype='float')})\n",
    "\n",
    "for date in dates:\n",
    "\n",
    "    gamma = gamma_omega_yearly.get(date)[0]\n",
    "    omega = gamma_omega_yearly.get(date)[1]\n",
    " \n",
    "    # filter expenditures for the current date\n",
    "    expenditures_date = bea_expenditures[bea_expenditures['date'] == date][['products', 'Expenditures']].set_index('products')\n",
    "    expenditures_date = expenditures_date.sort_index()\n",
    "    expenditures_date = expenditures_date.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    gamma_series = gamma[\"Row_Sum\"]\n",
    "    diag_matrix = np.diag(gamma_series)\n",
    "\n",
    "    x = np.identity(len(omega)) - (omega.T @ diag_matrix)\n",
    "\n",
    "    sales_date = np.linalg.inv(x) @ expenditures_date\n",
    "\n",
    "    sales_date['date'] = date\n",
    "    sales_date['products'] = expenditures_date.index\n",
    "    sales_date.rename(columns={'Expenditures': 'sales'}, inplace=True)\n",
    "\n",
    "    sales = pd.concat([sales, sales_date], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Prices of Intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prices of intermediates (we use cobb douglas production with intermediates)\n",
    "intermediates = pd.DataFrame({'date': pd.Series(dtype='datetime64[ns]'),\n",
    "                   'products': pd.Series(dtype='str'),\n",
    "                   'intermediates': pd.Series(dtype='float')})\n",
    "for date in dates:\n",
    "    prices_date = bea_prices[bea_prices['date'] == date][['products', 'Prices']].set_index('products')\n",
    "    prices_date = prices_date.sort_index()\n",
    "\n",
    "    gamma = gamma_omega_yearly.get(date)[0]\n",
    "    omega = gamma_omega_yearly.get(date)[1]\n",
    "    \n",
    "    for i in gamma.index:\n",
    "        log_prices = np.log(prices_date['Prices'])\n",
    "        log_prices.replace(-np.inf, 0, inplace=True)        \n",
    "        intermediates.loc[len(intermediates)] = [date, i, np.exp(omega.loc[i] @ log_prices)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Price of Value Added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prices of value added\n",
    "value_added = pd.DataFrame({'date': pd.Series(dtype='datetime64[ns]'),\n",
    "                   'products': pd.Series(dtype='str'),\n",
    "                   'value_added': pd.Series(dtype='float')})\n",
    "for date in dates:\n",
    "    # filter prices for the current date\n",
    "    prices_date = bea_prices[bea_prices['date'] == date][['products', 'Prices']].set_index('products')\n",
    "    prices_date = prices_date.sort_index()\n",
    "\n",
    "    # filter intermediates for the current date\n",
    "    intermediates_date = intermediates[intermediates['date'] == date][['products', 'intermediates']].set_index('products')\n",
    "    intermediates_date = intermediates_date.sort_index()\n",
    "\n",
    "    gamma = gamma_omega_yearly.get(date)[0]    \n",
    "    gamma_series = gamma[\"Row_Sum\"]\n",
    "\n",
    "    value_added_date = np.exp((1/(1 - gamma_series.sort_index()))*(np.log(prices_date['Prices']) - gamma_series.sort_index() * np.log(intermediates_date['intermediates'])))\n",
    "\n",
    "    value_added_date = value_added_date.reset_index().rename(columns={0: 'value_added'})\n",
    "    value_added_date['date'] = date\n",
    "    value_added_date.rename(columns={\"index\": \"products\"}, inplace=True)\n",
    "\n",
    "    value_added = pd.concat([value_added, value_added_date], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sales and Value Added VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 12\n",
    "residuals_part = []\n",
    "\n",
    "for product in gamma.index:\n",
    "    product_bad_dates = products_with_zero_somwhere[products_with_zero_somwhere[\"products\"] == product]\n",
    "    calculated = pd.merge(left=value_added, right=sales, on=['products', 'date'], how='inner')\n",
    "    calculated = calculated[calculated['products'] == product][['date', 'value_added', 'sales']].sort_values(['date'])\n",
    "    calculated = calculated.set_index('date')\n",
    "\n",
    "    if not product_bad_dates.empty: \n",
    "        calculated = calculated[~calculated.index.isin(product_bad_dates['date'])] # remove rows before we have all time series data\n",
    "    \n",
    "    calculated['value_added'] = np.log(calculated['value_added']).diff()\n",
    "    calculated['sales'] = np.log(calculated['sales']).diff()\n",
    "\n",
    "    calculated.replace([np.inf, -np.inf, np.nan], 0, inplace=True)\n",
    "\n",
    "    full_index_calc = pd.date_range(start=calculated.index.min(), end=calculated.index.max(), freq='MS')\n",
    "    calculated = calculated.reindex(full_index_calc)\n",
    "\n",
    "    model_calculated = VAR(calculated)\n",
    "    result_calculated = model_calculated.fit(lags)\n",
    "\n",
    "    residuals_calculated = result_calculated.resid.reset_index()\n",
    "    residuals_calculated['products'] = product\n",
    "    \n",
    "    residuals_part.append(residuals_calculated)\n",
    "\n",
    "IO_residuals = pd.concat(residuals_part, ignore_index=True)\n",
    "IO_residuals.rename(columns={'index': 'date', 'value_added': 'residual_value_added', 'sales': 'residual_sales'}, inplace=True)\n",
    "IO_residuals = IO_residuals.groupby(['date', 'products']).sum(min_count=1).reset_index()\n",
    "IO_residuals = IO_residuals.sort_values(['date', 'products'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Price and Quantity VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lags = 12\n",
    "residual_temp = []\n",
    "\n",
    "for product in gamma.index:\n",
    "\n",
    "    original = bea_PQE_merged[bea_PQE_merged['products'] == product][['date', 'Prices', 'Quantities']].sort_values(['date'])\n",
    "    original = original.set_index('date')\n",
    "\n",
    "    original.dropna(inplace=True)\n",
    "\n",
    "    original['Prices'] = np.log(original['Prices']).diff()\n",
    "    original['Quantities'] = np.log(original['Quantities']).diff()\n",
    "     \n",
    "\n",
    "    original.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    original.dropna(inplace=True)\n",
    "\n",
    "    full_index = pd.date_range(start=original.index.min(), end=original.index.max(), freq='MS')\n",
    "    original = original.reindex(full_index)\n",
    "\n",
    "    model_original = VAR(original)\n",
    "    result_original = model_original.fit(lags)\n",
    "\n",
    "    residuals_original = result_original.resid.reset_index()\n",
    "    residuals_original['products'] = product\n",
    "    \n",
    "    residual_temp.append(residuals_original)\n",
    "    \n",
    "residuals_normal = pd.concat(residual_temp, ignore_index=True)\n",
    "residuals_normal.rename(columns={'index': 'date', 'Prices': 'residual_prices', 'Quantities': 'residual_quantities'}, inplace=True)\n",
    "residuals_normal = residuals_normal.groupby(['date', 'products']).sum(min_count=1).reset_index()\n",
    "residuals_normal = residuals_normal.sort_values(['date', 'products'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a) Classification of Price of Value Added and Real Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_normal['majority_demand'] = ((residuals_normal['residual_prices'] * residuals_normal['residual_quantities']) >= 0).astype(int)\n",
    "residuals_normal['majority_supply'] = ((residuals_normal['residual_prices'] * residuals_normal['residual_quantities']) < 0).astype(int)\n",
    "\n",
    "IO_residuals['majority_demand_a'] = ((IO_residuals['residual_value_added'] * IO_residuals['residual_sales']) >= 0).astype(int)\n",
    "IO_residuals['majority_supply_a'] = ((IO_residuals['residual_value_added'] * IO_residuals['residual_sales']) < 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_IO = IO_residuals.pivot(index='products', columns='date', values='majority_demand_a')\n",
    "D_shapiro = residuals_normal.pivot(index='products', columns='date', values='majority_demand')\n",
    "D_IO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b) Calculate the Influence of D_i_t onto Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_o_yearly = {}\n",
    "\n",
    "for date in gamma_omega_yearly:\n",
    "    zero_products = all_bad_products_dict.get(date) # products that have no time series data for current month\n",
    "\n",
    "    current_delta = delta_final.copy()\n",
    "\n",
    "    for product in zero_products:\n",
    "        if product in delta_final.columns: \n",
    "            current_delta[product] = 0 # rescale omega for each month so it sums to 1 \n",
    "\n",
    "    current_delta = current_delta.values.astype(float)\n",
    "    big_o_current = np.linalg.inv(np.identity(len(current_delta)) - current_delta)\n",
    "    big_o_current = np.linalg.inv(big_o_current)\n",
    "    big_o_current = pd.DataFrame(big_o_current, index=delta_final.index, columns=delta_final.index)\n",
    "    big_o_current.index.name = None\n",
    "\n",
    "    for product in zero_products:\n",
    "        if product in big_o_current.columns: \n",
    "            big_o_current[product] = 0 # rescale omega for each month so it sums to 1 \n",
    "\n",
    "    big_o_yearly[date] = big_o_current      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D) Calculate theta_t "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_added_pivot = value_added.pivot(index='products', columns='date', values='value_added')\n",
    "\n",
    "# Dictionary that stores all the theta for each time period\n",
    "all_lambda = {}\n",
    "\n",
    "for series_name, series in value_added_pivot.items():\n",
    "    \"\"\"This function performs the steps outlined in 3d) to 3ef). It loops through P_VA for all time periods \n",
    "    and appends the resutling theta in all_thetas. \"\"\"\n",
    "    series[series == 0] = np.nan \n",
    "    \n",
    "    series = np.log(series)\n",
    "    \n",
    "    series = series.to_numpy().reshape(-1, 1)\n",
    "    \n",
    "    series = np.where(np.isnan(series), 0, series) # replacing NA with zeros\n",
    "    \n",
    "    ones = np.ones((1, len(series)))\n",
    "    theta = np.kron(series, ones)\n",
    "\n",
    "    current_big_o = big_o_yearly.get(series_name)\n",
    "\n",
    "    \"\"\"all columns are the same??? \"\"\"\n",
    "    theta_t = current_big_o @ theta \n",
    "\n",
    "    # Calculates and stores column sums \n",
    "    k = theta_t.sum(axis=0).to_frame().T\n",
    "\n",
    "    k = k.to_numpy().flatten() \n",
    "\n",
    "    inv_k = 1 / k\n",
    "    diag = np.diag(inv_k)\n",
    "    \n",
    "    lam = diag @ theta_t\n",
    "\n",
    "    lam.set_index(current_big_o.index, inplace=True)\n",
    "    lam.columns =current_big_o.index\n",
    "\n",
    "    all_lambda[f'{series_name}'] = lam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# g) + h) Calculate input D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_parts = {}\n",
    "\n",
    "for date in D_IO.columns: \n",
    "\n",
    "    D_IO_final = D_IO[str(date)]\n",
    "    missing_products = D_IO_final[D_IO_final.isna()].index\n",
    "    D_IO_final = D_IO_final.dropna()\n",
    "\n",
    "    lam_t = all_lambda.get(str(date))\n",
    "    lam_t = lam_t.drop(columns=missing_products)\n",
    "\n",
    "    D_t = lam_t.values @ D_IO_final.values\n",
    "    D_parts[date] = D_t\n",
    "\n",
    "D_matrix_final = pd.DataFrame(D_parts)\n",
    "\n",
    "D_matrix_final.set_index(current_big_o.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inflation and Weights for Graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annual_inflation_calculator_by_product(df):\n",
    "    # Ensure sorted by product and time\n",
    "    df = df.sort_values(by=['products', 'month'])\n",
    "    # Group by product and compute year-over-year inflation\n",
    "    df['inflation'] = df.groupby('products')['prices'].transform(lambda x: (x / x.shift(12) - 1) * 100)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = pd.read_excel(os.path.join(raw_data_path, 'BEA Monthly Prices.xlsx'))\n",
    "prices = clean_bea_PQE_table(prices, \"prices\")\n",
    "prices = prices.loc[prices.index.intersection(D_shapiro.index)]\n",
    "prices = prices[~prices.index.duplicated(keep='first')]\n",
    "prices = prices.reset_index()\n",
    "\n",
    "prices_long = pd.melt(prices, id_vars='products', var_name='month', value_name='prices')\n",
    "prices_long['prices'] = pd.to_numeric(prices_long['prices'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation = annual_inflation_calculator_by_product(prices_long)\n",
    "inflation = inflation.pivot(index='products', columns='month', values='inflation')\n",
    "inflation = inflation.iloc[:, 13:]\n",
    "\n",
    "bea_expenditures = bea_expenditures.pivot(index='products', columns='date', values='Expenditures')\n",
    "expenditure_weights = bea_expenditures.div(bea_expenditures.sum(axis=0), axis=1)\n",
    "expenditure_weights = expenditure_weights.iloc[:, 13:]\n",
    "weighted_inflation = expenditure_weights * inflation\n",
    "total_inflation = weighted_inflation.sum(axis=0)\n",
    "total_inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_matrix_final_percent = D_matrix_final / 100\n",
    "# inflation_contribution = {}\n",
    "\n",
    "# for date in D_matrix_final_percent.iloc[:, 1:]:\n",
    "#     print(date)\n",
    "#     current_inflation = total_inflation[total_inflation.index == date]\n",
    "#     D_matrix_final_percent[]\n",
    "#     contribution = \n",
    "    \n",
    "\n",
    "#     break\n",
    "# D_matrix_final_percent['scaled_A'] = D_matrix_final_percent[date] * current_inflation.iloc[0]\n",
    "# D_matrix_final_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation_parts_IO = {}\n",
    "\n",
    "for date in weighted_inflation:\n",
    "    current_demand = D_IO[str(date)]\n",
    "    current_supply = 1 - current_demand\n",
    "    current_inflation = weighted_inflation[str(date)]\n",
    "\n",
    "    demand_inflation = (current_inflation * current_demand).sum()\n",
    "    supply_inflation = (current_inflation * current_supply).sum()\n",
    "\n",
    "    inflation_parts_IO[date] = [demand_inflation, supply_inflation]\n",
    "\n",
    "inflation_IO_final = pd.DataFrame(inflation_parts_IO, index=['demand_inflation', 'supply_inflation'])\n",
    "inflation_IO_final = inflation_IO_final.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inflation_parts_shapiro = {}\n",
    "\n",
    "for date in weighted_inflation:\n",
    "    current_demand = D_shapiro[str(date)]\n",
    "    current_supply = 1 - current_demand\n",
    "    current_inflation = weighted_inflation[str(date)]\n",
    "\n",
    "    demand_inflation = (current_inflation * current_demand).sum()\n",
    "    supply_inflation = (current_inflation * current_supply).sum()\n",
    "\n",
    "    inflation_parts_shapiro[date] = [demand_inflation, supply_inflation]\n",
    "\n",
    "inflation_shapiro_final = pd.DataFrame(inflation_parts_shapiro, index=['demand_inflation', 'supply_inflation'])\n",
    "inflation_shapiro_final = inflation_shapiro_final.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(data, plot_title, plot_text):\n",
    "    data = data.loc[data.index >= pd.Timestamp('1970-01-01')]\n",
    "\n",
    "    supply_inflation = data[[\"supply_inflation\"]].copy()\n",
    "    supply_inflation.rename(columns={'supply_inflation': 'Supply Inflation'}, inplace=True)\n",
    "\n",
    "    demand_inflation = data[[\"demand_inflation\"]].copy()\n",
    "    demand_inflation.rename(columns={'demand_inflation': 'Demand Inflation'}, inplace=True)\n",
    "\n",
    "    supply_inflation['supply_pos'] = supply_inflation['Supply Inflation'].apply(lambda x: x if x > 0 else 0)\n",
    "    demand_inflation['demand_pos'] = demand_inflation['Demand Inflation'].apply(lambda x: x if x > 0 else 0)\n",
    "    supply_inflation['supply_neg'] = supply_inflation['Supply Inflation'].apply(lambda x: x if x < 0 else 0)\n",
    "    demand_inflation['demand_neg'] = demand_inflation['Demand Inflation'].apply(lambda x: x if x < 0 else 0)\n",
    "\n",
    "    demand_inflation = demand_inflation.iloc[:-1]\n",
    "    supply_inflation = supply_inflation.iloc[:-1]\n",
    "\n",
    "    plt.figure(figsize=(26, 12))\n",
    "\n",
    "    plt.stackplot(supply_inflation.index, demand_inflation['demand_pos'], supply_inflation['supply_pos'], colors= [\"#008000\", \"#FF0000\"], labels = [\"Deamnd\", \"Supply\"])\n",
    "    plt.stackplot(supply_inflation.index, demand_inflation['demand_neg'], supply_inflation['supply_neg'], colors= [\"#008000\", \"#FF0000\"])\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Inflation Percent')\n",
    "    plt.title(f'{plot_title}')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.text(0.02, 0.95, \n",
    "         f'{plot_text}', \n",
    "         transform=plt.gca().transAxes, fontsize=9,\n",
    "         bbox=dict(facecolor='white', alpha=0.8))\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(inflation_shapiro_final, \"Shapiro Classification\", \"• Using our 210 products \\n• Using Price and Quantities for Residuals and Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(inflation_IO_final, \"ShapirIO Classification\", \"• Using our 210 products \\n• Using Value-Added and Sales for Residuals and Classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_graph = plot_shapiro_graph_from_shapiro_ouput(shapiro_code_output, \"Shapiro Graph Using His 130 Products, Data as Found in His Paper\")\n",
    "shapiro_graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
