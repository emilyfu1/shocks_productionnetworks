{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import dotenv_values, find_dotenv\n",
    "import os\n",
    "from datacleaning.functions import filter_by_granularity\n",
    "from statsmodels.tsa.api import VAR\n",
    "# from statsmodels.tsa.stattools import adfuller\n",
    "# from statsmodels.tools.eval_measures import rmse, aic\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "# set path parameters\n",
    "config = dotenv_values(find_dotenv())\n",
    "path_rawdata = os.path.abspath(config[\"RAWDATA\"]) + '//'\n",
    "path_cleandata = os.path.abspath(config[\"CLEANDATA\"]) + '//'\n",
    "path_figures = os.path.abspath(config[\"FIGURES\"]) + '//'\n",
    "\n",
    "# for plots\n",
    "plt.rcParams.update({'font.size': 25})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the industry-by-industry I-O requirements table\n",
    "requirementstable = pd.read_pickle(path_cleandata + 'requirements_naics6.pkl')\n",
    "iorequirements_wide_fillna = requirementstable.fillna(value=0)\n",
    "\n",
    "# import pce data\n",
    "beadata = pd.read_pickle(path_cleandata + 'BEA_PCE.pkl')\n",
    "# products at 6th level of granularity\n",
    "beadata = filter_by_granularity(beadata, target_granularity=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a nice matrix\n",
    "\n",
    "# remove nans if any \n",
    "iorequirements_wide_fillna = requirementstable.fillna(value=0)\n",
    "\n",
    "# find sectors that show up as both buyers and sellers (to get a nice square matrix)\n",
    "iosectors = list(set(iorequirements_wide_fillna.index) & set(iorequirements_wide_fillna.columns))\n",
    "iorequirements_wide_fillna = iorequirements_wide_fillna[iosectors].loc[iosectors]\n",
    "# list of buyers and sellers (remove redundant sectors for which total expenses/sales are zero)\n",
    "inputsectors = [index for index in iorequirements_wide_fillna.index if iorequirements_wide_fillna.sum(axis=1)[index] != 0 and iorequirements_wide_fillna.sum(axis=0)[index] != 0]\n",
    "outputsectors = [column for column in iorequirements_wide_fillna.columns if iorequirements_wide_fillna.sum(axis=1)[column] != 0 and iorequirements_wide_fillna.sum(axis=0)[column] != 0]\n",
    "# get the intersection of these lists\n",
    "sectors_to_include = list(set(inputsectors) & set(outputsectors))\n",
    "\n",
    "# filter I-O table and residuals based on this list of sectors (to get a nice square matrix)\n",
    "iorequirements_wide_fillna.drop(columns=[col for col in iorequirements_wide_fillna.columns if col not in sectors_to_include], inplace=True)\n",
    "iorequirements_wide_fillna.drop(index=[idx for idx in iorequirements_wide_fillna.index if idx not in sectors_to_include], inplace=True)\n",
    "\n",
    "# sort sector names (honestly not necessary but it keeps me organized)\n",
    "iorequirements_wide_fillna = iorequirements_wide_fillna.sort_index().reindex(sorted(iorequirements_wide_fillna.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc_O\n",
      "Customs duties    1.0\n",
      "dtype: float64\n",
      "desc_I\n",
      "Community food, housing, and other relief services, including vocational rehabilitation services    1.0\n",
      "Customs duties                                                                                      1.0\n",
      "Death care services                                                                                 1.0\n",
      "Educational and vocational structures                                                               1.0\n",
      "Elementary and secondary schools                                                                    1.0\n",
      "Health care structures                                                                              1.0\n",
      "Home health care services                                                                           1.0\n",
      "Manufacturing structures                                                                            1.0\n",
      "Multifamily residential structures                                                                  1.0\n",
      "Office and commercial structures                                                                    1.0\n",
      "Other nonresidential structures                                                                     1.0\n",
      "Owner-occupied housing                                                                              1.0\n",
      "Power and communication structures                                                                  1.0\n",
      "Religious organizations                                                                             1.0\n",
      "Single-family residential structures                                                                1.0\n",
      "Tenant-occupied housing                                                                             1.0\n",
      "Transportation structures and highways and streets                                                  1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "requirements_inv = np.linalg.inv(iorequirements_wide_fillna)\n",
    "requirements_inv = pd.DataFrame(requirements_inv, index=iorequirements_wide_fillna.index, columns=iorequirements_wide_fillna.columns)\n",
    "\n",
    "requirements_inv_colsum = requirements_inv.sum(axis=0)\n",
    "requirements_inv_rowsum = requirements_inv.sum(axis=1)\n",
    "\n",
    "print(requirements_inv_colsum[requirements_inv_colsum >= 1])\n",
    "print(requirements_inv_rowsum[requirements_inv_rowsum >= 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desc_O\n",
      "Customs duties    0.0\n",
      "dtype: float64\n",
      "desc_I\n",
      "Community food, housing, and other relief services, including vocational rehabilitation services    0.0\n",
      "Customs duties                                                                                      0.0\n",
      "Death care services                                                                                 0.0\n",
      "Educational and vocational structures                                                               0.0\n",
      "Elementary and secondary schools                                                                    0.0\n",
      "Health care structures                                                                              0.0\n",
      "Home health care services                                                                           0.0\n",
      "Manufacturing structures                                                                            0.0\n",
      "Multifamily residential structures                                                                  0.0\n",
      "Office and commercial structures                                                                    0.0\n",
      "Other nonresidential structures                                                                     0.0\n",
      "Owner-occupied housing                                                                              0.0\n",
      "Power and communication structures                                                                  0.0\n",
      "Religious organizations                                                                             0.0\n",
      "Single-family residential structures                                                                0.0\n",
      "Tenant-occupied housing                                                                             0.0\n",
      "Transportation structures and highways and streets                                                  0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# get the industry-by-industry I-O matrix\n",
    "iomatrix_wide = np.identity(len(iorequirements_wide_fillna)) - np.linalg.inv(iorequirements_wide_fillna)\n",
    "iomatrix_wide = pd.DataFrame(iomatrix_wide, index=iorequirements_wide_fillna.index, columns=iorequirements_wide_fillna.columns)\n",
    "\n",
    "iomatrix_wide_colsum = iomatrix_wide.sum(axis=0)\n",
    "iomatrix_wide_rowsum = iomatrix_wide.sum(axis=1)\n",
    "\n",
    "print(iomatrix_wide_colsum[iomatrix_wide_colsum == 0])\n",
    "print(iomatrix_wide_rowsum[iomatrix_wide_rowsum == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a long form I-O matrix (i kinda don't know how to use the concordance otherwise)\n",
    "\n",
    "iomatrix_long = pd.melt(iomatrix_wide.reset_index(), id_vars=['desc_I'], var_name='desc_O', value_name='value')\n",
    "\n",
    "# now taking some steps to get a product-by-product version\n",
    "\n",
    "# import concordance\n",
    "concordance = pd.read_pickle(path_cleandata + 'concordance//concordance6_naics6.pkl')\n",
    "\n",
    "# change concordance column names so that i can merge with I-O table (first merge with sellers then merge with buyers)\n",
    "crosswalk_I = concordance.rename(columns={'product': 'product_I', 'NAICS_desc': 'desc_I'})\n",
    "crosswalk_O = concordance.rename(columns={'product': 'product_O', 'NAICS_desc': 'desc_O'})\n",
    "\n",
    "# merge crosswalk to sellers \n",
    "add_naics_I = pd.merge(left=crosswalk_I, right=iomatrix_long, on='desc_I', how='inner')[['product_I', 'desc_I', 'desc_O', 'value']]\n",
    "add_naics_O = pd.merge(left=crosswalk_O, right=iomatrix_long, on='desc_O', how='inner')[['product_O', 'desc_O', 'desc_I', 'value']]\n",
    "\n",
    "iomatrix_long = pd.merge(left=add_naics_I, right=add_naics_O, on=['desc_I', 'desc_O', 'value'], how='outer')\n",
    "\n",
    "# collapse by product_O\n",
    "iomatrix_long_byproduct = iomatrix_long[['product_I', 'desc_I', 'value', 'product_O']].groupby(['product_I', 'desc_I', 'product_O'], as_index=False).sum(min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import make table\n",
    "maketable_wide = pd.read_pickle(path_cleandata + 'make_naics6.pkl')\n",
    "maketable_wide = maketable_wide.fillna(value=0)\n",
    "\n",
    "# get sum of total sales for each industry\n",
    "sales = maketable_wide.sum(min_count=1, axis=1)\n",
    "\n",
    "# merge total sales with concordance (start to get by-product shares of sales to use as weights)\n",
    "sales_byproduct = pd.merge(left=sales.reset_index().rename(columns={0: 'sales'}), right=crosswalk_I, on='desc_I', how='inner')\n",
    "\n",
    "# get total sales for each product\n",
    "total_sales_by_product = sales_byproduct.groupby('product_I')['sales'].sum().reset_index()\n",
    "total_sales_by_product.columns = ['product_I', 'total_sales']\n",
    "sales_byproduct = pd.merge(left=sales_byproduct, right=total_sales_by_product, how='inner', on='product_I')\n",
    "\n",
    "# calculate each industry's share within each product category\n",
    "sales_byproduct['share_of_sales_by_industry'] = sales_byproduct['sales'] / sales_byproduct['total_sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge this shares column into the uhhhh\n",
    "iomatrix_long_byproduct = pd.merge(left=iomatrix_long_byproduct, right=sales_byproduct[['desc_I', 'product_I', 'share_of_sales_by_industry']], on=['product_I', 'desc_I'])\n",
    "\n",
    "# find weighted sum based on share of make table sales that we calculated\n",
    "iomatrix_long_byproduct['weightTimesDeltaValue'] = iomatrix_long_byproduct['value'] * iomatrix_long_byproduct['share_of_sales_by_industry']\n",
    "\n",
    "# fully collapse to product level\n",
    "iomatrix_long_byproduct = iomatrix_long_byproduct[['product_I', 'value', 'product_O']].groupby(['product_I', 'product_O'], as_index=False).sum(min_count=1)\n",
    "\n",
    "# here's a square version\n",
    "iomatrix_wide_byproduct = iomatrix_long_byproduct.pivot_table(index='product_I', columns='product_O', values='value', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that we have the same products in all the data\n",
    "\n",
    "# first, i need products that actually have a full series so we can run the VARs\n",
    "# total number of unique dates\n",
    "total_dates = beadata['date'].nunique()\n",
    "# group by product and count the number of unique dates for each product\n",
    "product_dates_count = beadata.dropna().groupby('product')['date'].nunique()\n",
    "# filter products that appear at all dates\n",
    "products_appear_all_dates = product_dates_count[product_dates_count == total_dates].index.tolist()\n",
    "\n",
    "# next i deal with the I-O matrix products...\n",
    "# list of buyers (the whole usetable_wide_fillna.sum(axis=1/axis=0)[index] != 0 thing is so that i dont get any division by 0)\n",
    "inputproducts = [index for index in iomatrix_wide_byproduct.index if iomatrix_wide_byproduct.sum(axis=1)[index] != 0 and iomatrix_wide_byproduct.sum(axis=0)[index] != 0]\n",
    "# list of sellers\n",
    "outputproducts = [column for column in iomatrix_wide_byproduct.columns if iomatrix_wide_byproduct.sum(axis=1)[column] != 0 and iomatrix_wide_byproduct.sum(axis=0)[column] != 0]\n",
    "\n",
    "# final list of products\n",
    "products_to_include = list(set(beadata['product']) & set(iomatrix_wide_byproduct.index) & set(iomatrix_wide_byproduct.columns))\n",
    "products_to_include.sort()\n",
    "\n",
    "# filter for products based on this list\n",
    "iomatrix_wide_byproduct = iomatrix_wide_byproduct[products_to_include]\n",
    "iomatrix_wide_byproduct = iomatrix_wide_byproduct.loc[iomatrix_wide_byproduct.index.isin(products_to_include)]\n",
    "beadata = beadata.loc[beadata['product'].isin(products_to_include)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then this sum gives the share, for each sector, of what is used in intermediates\n",
    "intermediate_salesshares = iomatrix_wide_byproduct.sum(axis=1)\n",
    "\n",
    "# now get intermediate cost shares...\n",
    "intermediate_costshares = (iomatrix_wide_byproduct @ np.diag(1/intermediate_salesshares)).T\n",
    "\n",
    "# fix index names and stuff which is annoying\n",
    "intermediate_costshares.index = intermediate_salesshares.index\n",
    "intermediate_salesshares = intermediate_salesshares.rename_axis('product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product\n",
       "Accessories and parts                                              12.126869\n",
       "Air transportation                                                  1.025057\n",
       "Alcohol in purchased meals                                          6.305685\n",
       "Amusement parks, campgrounds, and related recreational services     4.288247\n",
       "Audio equipment                                                     2.285304\n",
       "                                                                     ...    \n",
       "Trust, fiduciary, and custody activities                            1.358800\n",
       "Veterinary and other services for pets                              1.165749\n",
       "Video and audio streaming and rental                                2.999380\n",
       "Video discs, tapes, and permanent digital downloads                 1.085186\n",
       "Women's and girls' clothing                                         4.491039\n",
       "Length: 103, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so..... with the way that we are using the requirements matrix\n",
    "# to do all our calculations now i am still noticing all these sectors \n",
    "# that have negative value added\n",
    "\n",
    "intermediate_salesshares[intermediate_salesshares > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get separate series for prices, quantities, expenditures to save\n",
    "prices = beadata[['product', 'date', 'priceindex']]\n",
    "quantities = beadata[['product', 'date', 'quantityindex']]\n",
    "expenditures = beadata[['product', 'date', 'expenditures']]\n",
    "# prices.to_pickle(path_cleandata + 'inversions//prices.pkl')\n",
    "# quantities.to_pickle(path_cleandata + 'inversions//quantities.pkl')\n",
    "# expenditures.to_pickle(path_cleandata + 'inversions//expenditures.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all time periods\n",
    "\n",
    "# i want to get dates where there is data for everything\n",
    "dates = list(set(prices['date'].unique()) & set(quantities['date'].unique()) & set(expenditures['date'].unique()))\n",
    "dates.sort()\n",
    "\n",
    "# choose one date per decade for the troubleshooting plots (maybe just the fifth year, July 31 per decade since its the middle of the year ish)\n",
    "plotdates = [dt.datetime(1965,7,31), dt.datetime(1975,7,31), dt.datetime(1985,7,31), dt.datetime(1995,7,31), dt.datetime(2005,7,31), dt.datetime(2015,7,31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate sales in product and time period\n",
    "sales = pd.DataFrame({'date': pd.Series(dtype='datetime64[ns]'),\n",
    "                   'product': pd.Series(dtype='str'),\n",
    "                   'sales': pd.Series(dtype='float')})\n",
    "for date in dates:\n",
    "    # filter expenditures for the current date\n",
    "    expenditures_date = expenditures[expenditures['date'] == date][['product', 'expenditures']].set_index('product')\n",
    "    expenditures_date = expenditures_date.sort_index()\n",
    "\n",
    "    # Create the diagonal matrix from intermediate_salesshares\n",
    "    diag_matrix = np.diag(intermediate_salesshares)\n",
    "    \n",
    "    # calculate sales for each product\n",
    "    # intermediate_costshares.T @ diag(intermediate_salesshares) gets you the 2017 I-O matrix 2017 (inputs needed from one product to another to produce one unit of output)\n",
    "    # we're assuming the same I-O matrix for each time period, so now we want sales in each time period using this I-O matrix!\n",
    "    # (inputs needed from one product to another to produce one unit of output) x (expenditures for each product at time t) gets you sales at time t for each product\n",
    "    sales_date = np.linalg.inv(np.identity(len(intermediate_costshares)) - (intermediate_costshares.T @ diag_matrix)) @ expenditures_date\n",
    "\n",
    "    # set some columns to append\n",
    "    sales_date['date'] = date\n",
    "    sales_date['product'] = expenditures_date.index\n",
    "    sales_date.rename(columns={'expenditures': 'sales'}, inplace=True)\n",
    "    # append\n",
    "    sales = pd.concat([sales, sales_date], ignore_index=True)\n",
    "\n",
    "# sales.to_pickle(path_cleandata + 'inversions//sales.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prices of intermediates (we use cobb douglas production with intermediates)\n",
    "intermediates = pd.DataFrame({'date': pd.Series(dtype='datetime64[ns]'),\n",
    "                   'product': pd.Series(dtype='str'),\n",
    "                   'intermediates': pd.Series(dtype='float')})\n",
    "for date in dates:\n",
    "    # filter prices for the current date\n",
    "    prices_date = prices[prices['date'] == date][['product', 'priceindex']].set_index('product')\n",
    "    prices_date = prices_date.sort_index()\n",
    "    \n",
    "    for i in products_to_include:\n",
    "        intermediates.loc[len(intermediates)] = [date, i , np.exp(intermediate_costshares.loc[i] @ np.log(prices_date['priceindex']))]\n",
    "\n",
    "# intermediates.to_pickle(path_cleandata + 'inversions//intermediateprices.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate prices vs output prices (plot)\n",
    "\n",
    "compare = pd.merge(left=intermediates, right=prices, on=['date', 'product'], how='inner')\n",
    "compare = compare[compare['date'].isin(dates)].sort_values(['date', 'priceindex'], ascending=False)\n",
    "\n",
    "for idx, date in enumerate(plotdates):\n",
    "    toplot_date = compare[compare['date'] == date].set_index('product')\n",
    "    products = toplot_date.index\n",
    "    price = np.log10(toplot_date['priceindex'])\n",
    "    intermediates_forplot = np.log10(toplot_date['intermediates'])\n",
    "    diff = price - intermediates_forplot\n",
    "\n",
    "    datestr = date.strftime(\"%Y/%m/%d\").replace('/', '')\n",
    "\n",
    "    # plots for price residuals\n",
    "\n",
    "    # calculate heights for stacked bars\n",
    "    positive_diff = np.where((diff >= 0) & (intermediates_forplot >= 0), diff, 0)\n",
    "    positive_adjusted = np.where((diff >= 0) & (intermediates_forplot >= 0), intermediates_forplot, intermediates_forplot)\n",
    "    negative_diff = np.where((diff < 0) & (intermediates_forplot < 0), diff, 0)\n",
    "    negative_adjusted = np.where((diff < 0) & (intermediates_forplot < 0), intermediates_forplot, intermediates_forplot)\n",
    "\n",
    "    # open figure\n",
    "    fig, ax = plt.subplots(figsize=(60, 30))\n",
    "\n",
    "    ax.axhline(y=0, color='black')\n",
    "\n",
    "    # layering bars for the right display\n",
    "    ax.bar(products, diff, color='salmon', label='Difference between price indexes and prices of intermediates')\n",
    "    ax.bar(products, positive_adjusted, color='skyblue', label='Intermediates')\n",
    "    ax.bar(products, positive_diff, color='salmon', bottom=positive_adjusted, label='_')\n",
    "    ax.bar(products, negative_adjusted, color='skyblue', label='_')\n",
    "    ax.bar(products, negative_diff, color='salmon', bottom=negative_adjusted, label='_')\n",
    "\n",
    "    line, = ax.plot(products, price, linestyle='-', marker='o', color='black', linewidth=4, label='Price index')\n",
    "    \n",
    "    # set appearances\n",
    "    ax.set_title('Prices, intermediates ' + datestr)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_ylabel('(log10) Values')\n",
    "    ax.set_xticks(np.arange(len(products))) \n",
    "    ax.set_xticklabels(products, rotation=90)\n",
    "    ax.set_xlim(-1, len(products))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # plt.savefig(path_figures + 'prices\\\\log_intermediates_' + datestr + '.pdf', bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intermediate prices vs output prices (plot, non-log scale)\n",
    "\n",
    "for idx, date in enumerate(plotdates):\n",
    "    toplot_date = compare[compare['date'] == date].set_index('product')\n",
    "    products = toplot_date.index\n",
    "    price = toplot_date['priceindex']\n",
    "    intermediates_forplot = toplot_date['intermediates']\n",
    "    diff = price - intermediates_forplot\n",
    "\n",
    "    datestr = date.strftime(\"%Y/%m/%d\").replace('/', '')\n",
    "\n",
    "    # plots for price residuals\n",
    "\n",
    "    # calculate heights for stacked bars\n",
    "    positive_diff = np.where((diff >= 0) & (intermediates_forplot >= 0), diff, 0)\n",
    "    positive_adjusted = np.where((diff >= 0) & (intermediates_forplot >= 0), intermediates_forplot, intermediates_forplot)\n",
    "    negative_diff = np.where((diff < 0) & (intermediates_forplot < 0), diff, 0)\n",
    "    negative_adjusted = np.where((diff < 0) & (intermediates_forplot < 0), intermediates_forplot, intermediates_forplot)\n",
    "\n",
    "    # open figure\n",
    "    fig, ax = plt.subplots(figsize=(60, 30))\n",
    "\n",
    "    ax.axhline(y=0, color='black')\n",
    "\n",
    "    # layering bars for the right display\n",
    "    ax.bar(products, diff, color='salmon', label='Difference between price indexes and prices of intermediates')\n",
    "    ax.bar(products, positive_adjusted, color='skyblue', label='Intermediates')\n",
    "    ax.bar(products, positive_diff, color='salmon', bottom=positive_adjusted, label='_')\n",
    "    ax.bar(products, negative_adjusted, color='skyblue', label='_')\n",
    "    ax.bar(products, negative_diff, color='salmon', bottom=negative_adjusted, label='_')\n",
    "\n",
    "    line, = ax.plot(products, price, linestyle='-', marker='o', color='black', linewidth=4, label='Price index')\n",
    "    \n",
    "    # set appearances\n",
    "    ax.set_title('Prices, intermediates ' + datestr)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_ylabel('Values')\n",
    "    ax.set_xticks(np.arange(len(products))) \n",
    "    ax.set_xticklabels(products, rotation=90)\n",
    "    ax.set_xlim(-1, len(products))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # plt.savefig(path_figures + 'prices\\\\intermediates_' + datestr + '.pdf', bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate prices of value added\n",
    "value_added = pd.DataFrame({'date': pd.Series(dtype='datetime64[ns]'),\n",
    "                   'product': pd.Series(dtype='str'),\n",
    "                   'value_added': pd.Series(dtype='float')})\n",
    "for date in dates:\n",
    "    # filter prices for the current date\n",
    "    prices_date = prices[prices['date'] == date][['product', 'priceindex']].set_index('product')\n",
    "    prices_date = prices_date.sort_index()\n",
    "\n",
    "    # filter intermediates for the current date\n",
    "    intermediates_date = intermediates[intermediates['date'] == date][['product', 'intermediates']].set_index('product')\n",
    "    intermediates_date = intermediates_date.sort_index()\n",
    "\n",
    "    value_added_date = np.exp((1/(1 - intermediate_salesshares))*(np.log(prices_date['priceindex']) - intermediate_salesshares * np.log(intermediates_date['intermediates'])))\n",
    "\n",
    "    value_added_date = value_added_date.reset_index().rename(columns={0: 'value_added'})\n",
    "    value_added_date['date'] = date\n",
    "\n",
    "    value_added = pd.concat([value_added, value_added_date], ignore_index=True)\n",
    "\n",
    "# value_added.to_pickle(path_cleandata + 'inversions//valueaddedprices.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value added prices vs output prices (plot)\n",
    "\n",
    "compare = pd.merge(left=value_added, right=prices, on=['date', 'product'], how='inner')\n",
    "compare = compare[compare['date'].isin(dates)].sort_values(['date', 'priceindex'], ascending=False)\n",
    "\n",
    "for idx, date in enumerate(plotdates):\n",
    "    toplot_date = compare[compare['date'] == date].set_index('product')\n",
    "    products = toplot_date.index\n",
    "    price = np.log10(toplot_date['priceindex'])\n",
    "    value_added_forplot = np.log10(toplot_date['value_added'])\n",
    "    diff = price - value_added_forplot\n",
    "\n",
    "    datestr = date.strftime(\"%Y/%m/%d\").replace('/', '')\n",
    "\n",
    "    # plots for price residuals\n",
    "\n",
    "    # calculate heights for stacked bars\n",
    "    positive_diff = np.where((diff >= 0) & (value_added_forplot >= 0), diff, 0)\n",
    "    positive_adjusted = np.where((diff >= 0) & (value_added_forplot >= 0), value_added_forplot, value_added_forplot)\n",
    "    negative_diff = np.where((diff < 0) & (value_added_forplot < 0), diff, 0)\n",
    "    negative_adjusted = np.where((diff < 0) & (value_added_forplot < 0), value_added_forplot, value_added_forplot)\n",
    "\n",
    "    # open figure\n",
    "    fig, ax = plt.subplots(figsize=(60, 30))\n",
    "\n",
    "    ax.axhline(y=0, color='black')\n",
    "\n",
    "    # layering bars for the right display\n",
    "    ax.bar(products, diff, color='salmon', label='Difference between price indexes and value added')\n",
    "    ax.bar(products, positive_adjusted, color='skyblue', label='Value added')\n",
    "    ax.bar(products, positive_diff, color='salmon', bottom=positive_adjusted, label='_')\n",
    "    ax.bar(products, negative_adjusted, color='skyblue', label='_')\n",
    "    ax.bar(products, negative_diff, color='salmon', bottom=negative_adjusted, label='_')\n",
    "\n",
    "    line, = ax.plot(products, price, linestyle='-', marker='o', color='black', linewidth=4, label='Price index')\n",
    "    \n",
    "    # set appearances\n",
    "    ax.set_title('Prices, value added ' + datestr)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.set_ylabel('(log10) Values')\n",
    "    ax.set_xticks(np.arange(len(products))) \n",
    "    ax.set_xticklabels(products, rotation=90)\n",
    "    ax.set_xlim(-1, len(products))\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # plt.savefig(path_figures + 'prices\\\\log_valueadded_' + datestr + '.pdf', bbox_inches='tight')\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
